---
title: "slides day 3"
author: "Christopher M. Loan, MS"
date: 'February 22, 2022'
output: xaringan::moon_reader
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Workshop Progress

On Day 1, we discussed:
  * what DTs are
  * how to fit DTs
  * how to interprate/visualize DTs
  * how to evaluate DTs

On Day 2, we discussed
  
  * what RFs are
  * how to fit RFs
  * how to evaluate RFs
  * hyperparameters
  * how hyperparameters are used to improve models
  
---

# Looking Ahead

Today, we will discuss:

* Grid Search for Tuning Hyperparameters
* Cross validation
* Interpretation of Random Forests
* More Advanced Visualizations (Partial Dependency Plots)

---

# set up

```{r}
outcome_next_year <- 'se_xpd_totl_gd_zs'
library(tidyverse)
library(ranger)
library(rsample)
library(dials)
library(kableExtra)
library(emojifont)
source(here::here('scripts/functions.R'))
```

---

# set up

Let's go ahead and get things set up like yesterday:


```{r}
key <- 
  read.csv(here::here('data/keys', 'key_impute.csv'))

dat <- 
  read.csv(
    here::here(
    'data', 
    'se_asia_imputed_world_bank.csv'
    ))


set.seed(022023)
sea_split <- initial_split(dat)
sea_training <- training(sea_split) 
sea_testing <- testing(sea_split) 
```

```{r include = FALSE, echo = FALSE}
otcm <-  
  key %>% 
  filter(janitor_version == outcome_next_year) %>% 
  pull(indicator_name)
```

Outcome =  <u>next year's</u> `r otcm`

Units = `r str_extract(otcm, '\\([^\\)]+\\)')`

---

# let's establish two "benchmarks" to beat

## Benchmark 1: guess the average

Guessing the mean or median has common-sense validity.

The most common-sense approach is to guess the average (mean or median) `r paste0(outcome_next_year)` this year to be next year's value.

If a model does not have a better RMSE than this, it is not worth using.

```{r include=FALSE}
naive_guess.median <- median(sea_training[,outcome_next_year])
naive_guess.mean <- mean(sea_training[,outcome_next_year])
```

---

# Benchmark 2: Group-Averages

```{r}
group_averages <- 
  sea_training %>% 
  group_by(country_name) %>% 
  summarize(
    predictions_group.median = median(outcome_next_year),
    predictions_group.mean = mean(outcome_next_year)
  ) %>% 
  ungroup()
```


---
# Benchmark 3: Default RF

```{r}
rf_default <- 
  ranger(
    outcome_next_year ~ ., 
    data = sea_training,
    importance = 'permutation'
  )
```

---

## A Note on RMSE

The nice thing about RMSE is that it is in the units of the outcome. 

RMSE is the average error in the units of the outcome. 

$RMSE = \sqrt{\frac{\Sigma (y_{actual} - y_{predicted})^2}{N}}$

For us, this is `r str_extract(otcm, '\\([^\\)]+\\)')`

---

# Benchmark RMSEs

```{r}
benchmark_rmse <- 
  sea_testing %>% 
  full_join(group_averages) %>% 
  transmute(
    !!sym(outcome_next_year),
    predictions_group.median,
    predictions_group.mean,
    prediction_default.rf = predict(rf_default, .)$prediction
    ) %>% 
  summarize(
    rmse_guess.median = 
      rmse(observed_y = !!sym(outcome_next_year), predicted_y = naive_guess.median),
    rmse_guess.mean = 
      rmse(observed_y = !!sym(outcome_next_year), predicted_y = naive_guess.mean),
    rmse_guess.group.median = 
      rmse(observed_y = !!sym(outcome_next_year), predicted_y = predictions_group.median),
    rmse_guess.group.mean = 
      rmse(observed_y = !!sym(outcome_next_year), predicted_y = predictions_group.mean),
    rmse_default.rf = 
      rmse(observed_y = !!sym(outcome_next_year), predicted_y = prediction_default.rf)
  )
```

---

# Benchmarks

Well, it actually looks like our group means are outperforming out default random forest. 

```{r include = TRUE, echo = FALSE}
benchmark_rmse %>% 
  pivot_longer(
    cols = everything(),
    names_prefix = 'rmse_', 
    names_to = 'rmse'
    ) %>% 
  kbl(caption = 'Fit of various benchmarks', digits = 2)
```

The training mean by country better predicts the outcome than a default random forest. 

In this case, we either need to tune the model to outperform this, or else there is no reason to not take the group mean (unless you want to see VIPs, PDPs, etc)

To put this in standard "research jargon", the between-country variance is higher than within

---

# Performance of Averages

Let's actually look how bad the average guess would be for each country

```{r echo=FALSE, include=TRUE}
sea_testing %>% 
  ggplot(
    aes(
      y = !!sym(outcome_next_year),
      x = year,
      color = country_name
    )
  ) +
  geom_segment(
    position = pos_tmp, 
    aes(xend = year),
    yend = naive_guess.mean,
    ) +
  geom_point(
    alpha = 0.4,
    position = pos_tmp, 
    aes(size = abs(!!sym(outcome_next_year) - naive_guess.mean)),
  ) +
  geom_hline(
    yintercept = naive_guess.mean,
    linetype = 2,
    size = 1.2, 
    alpha = 0.4
  ) +
  theme_bw() +
  facet_wrap(
    vars(
      country_name
    )
  ) +
  labs(
    y = pstr,
    size = 'Absolute\nDifference\nMean',
    fill = element_blank(),
    title = '(In)Adequecy of mean across Group'
  ) +
  theme(
    plot.title.position = 'plot',
    legend.position = c(0.8, 0.14)
  ) +
  guides(color = 'none')
```

---


# Performance of Averages

We can get a decent estimate for a mean by group, actually.

```{r echo=FALSE, include=TRUE}
sea_testing %>% 
  full_join(group_averages) %>% 
  ggplot(
    aes(
      y = !!sym(outcome_next_year),
      x = year,
      color = country_name
    )
  ) +
  geom_segment(
    position = pos_tmp, 
    aes(
      xend = year,
      yend = predictions_group.mean
      )
    ) +
  geom_point(
    alpha = 0.4,
    position = pos_tmp, 
    aes(
      size = abs(!!sym(outcome_next_year) - predictions_group.mean),
      ),
  ) +
  geom_hline(
    aes(yintercept = predictions_group.mean),
    linetype = 2,
    size = 1.2, 
    alpha = 0.4
  ) +
  theme_bw() +
  facet_wrap(
    vars(
      country_name
    )
  ) +
  labs(
    y = pstr,
    size = 'Absolute\nDifference\nMean',
    fill = element_blank(),
    title = 'Adequecy of Mean within Group'
  ) +
  theme(
    plot.title.position = 'plot',
    legend.position = c(0.8, 0.14)
  ) +
  guides(color = 'none')
```



---

class: center, middle

# Onward & Upward

Now that we've established these benchmarks, let's talk hyperparameters again

---

# Selecting Hyperparameters

Some have called tuning & hyperparameter selection as much of an art as a science.

This is likely true when you're applying the basic methods we're going to cover today. 

Some automated methods are much more systematic:

* particle swarm optimization (PSO)
* genetic algorithm (GA)

Despite being more systematic:

* such methods require deeper knowledge
* most still require human decision-points
* some have their own hyper-hyperparameters!

**This is an entire field; we are only going to discuss grid search today**

---

# Grid Search

Purpose:
* (technical) Calibrate the underlying algorithms to your data
* (practical) Have the most accurate predictions 

Steps:
* make a grid of possible hyperparameters
* train models using those constraints
* estimate accuracy - MSE / classification accuracy / etc. - in predicting the outcome with unseen data for each model 

___

## Wait ... does this sound familiar?

---

# Refresher

When I demonstrated what hyperparameters are, I actually conducted 2 grid searches:
  * `num.trees`
  * `mtry`

This did not include a grid search of all possible values

---

# Let's make a grid

```{r}
tree_sizes <- 
  seq(from = 500, to = 4000, by = 150)

mtry_values <- 
  seq(from = 1, to = ncol(sea_testing)-1, 2)

all_params <- 
  expand_grid(num.tree = tree_sizes, mtry = mtry_values)

dim(all_params)
```

---

# loop through

```{r echo = FALSE, include = TRUE, eval = FALSE}
rf_list <- vector('list', nrow(all_params))

for (i in 1:length(rf_list)){
  rf_list[[i]] <- 
    ranger(
      outcome_next_year ~ ., 
      data = sea_training,
      mtry = all_params$mtry[i],
      num.trees = all_params$num.tree[i],
      num.threads = 8L
    )
  print(paste0('iteration #', i, ' complete'))
}
```


```{r echo = FALSE, include = TRUE, eval = FALSE}
OOB_heatmap <- 
  all_params %>% 
  mutate(
    prediction_error = map_dbl(rf_list, ~.x$prediction.error),
    min = if_else(prediction_error == min(prediction_error), 'minimum', ''), 
    pe_label = rbinom(nrow(.), 1, 0.1),
    pe_label = if_else(pe_label == 1 | min == 'minimum', round(prediction_error, 3), NA_real_)
  ) %>% 
  ggplot(
    aes(
      x = mtry,
      y = num.tree,
      fill = prediction_error
      )
  ) + 
  geom_tile() + 
  geom_label(
    aes(
      color = min, 
      label = pe_label
      ), 
    show.legend = FALSE
    ) +
  coord_cartesian() + 
  scale_color_grey(start = 0.6, end = 1) +
  theme_minimal(base_size = 30) +
  labs(
    title = 'OOB Prediction Error by mtry and num.trees', 
    x = 'mtry',
    y = 'num.tree',
    fill = 'Mean Square Error (MSE)',
    caption = 'OOB Error (MSE) printed in each cell\nRandom sampling of OOB printed'
  ) +
  theme(legend.position = 'bottom')

ggsave(
  filename = here::here('imgs', 'day_3_OOB_heatmap.png'),
  plot = OOB_heatmap,
  )
```


```{r echo = FALSE, include = TRUE}
knitr::include_graphics(
  here::here('imgs', 'day_3_OOB_heatmap.png')
)
```

---

# Predictions are pretty flat... 

As you can see, the best value isn't that much better than the worst.


On day 1, what did I say were the 3 ways to improve a model?

1. a model
2. user-specified settings (**hyperparameters**)
3. data

Because hyperparameters are evaluated on data, we actually can only find optimal settings **with respect to the training and testing data**.

By random chance, what if the training and testing sets were biased towards different settings?

This could lead to an algorithm to perform poorly, even though we conducted hyperparameter tuning.

---

# other issues with tuning via grid search

Before turning our intention to improving the data used to train, we should talk about two other issues with grid search: 

* local minimums, unless
  * you cover the **entire** space 
  * you get lucky `r emoji('four_leaf_clover')`

* computation time increases with number of
  * hyperparameters
  * possible hyperparameter values

**Spoiler:** we're not going to cover local minimums in this workshop. Consider independent research at a later date.

---

# Increasing the variation in predictions

k-fold (a.k.a v-fold) cross-validation CV:

* improves the issues with flat predictions.
* makes of our training data to both train and test the model.
* allows variance around fit to be assessed (e.g., standard error of OOB error)
* is done **after we withhold final testing data**

The logic is a mixture of resampling and withholding for training and testing (done before):

* the data withheld here is called the "fold testing data" or "assessment data"
* the data used to train the model is called "fold training" or "analysis data"

---

# k-fold Cross Validation

Process: 
* split the training data into "k" pieces.
* train a model with a proportion of the data ($n_{train} = \frac{k-1}{k}*N_{total}$)
* assess prediction accuracy with withheld ($n_{assess} = \frac{1}{k}*N_{total}$)

For example:`n` = 1000 *observations* & `k` = 10 k-fold CV
* Each fold has `n`/10 = 1000/10 = 100 observations
* Model fit for a given hyperparameter is determined

---

# Diagram


```{r include = TRUE, echo = FALSE}
diagram_dat <- data.frame(matrix(rep('analysis', 100), nrow = 10, ncol = 10))

for (i in 1:10){
  diagram_dat[i, i] <- 'assessment'
}

diagram_dat %>% 
  mutate(
    row_id = 1:n()
  ) %>% 
  pivot_longer(cols = -row_id) %>% 
  mutate(
    name = as.numeric(gsub('X', '', name)),
    unseen = if_else(value == 'FOLD\nTESTING', 'UNSEEN', '')
    ) %>% 
  ggplot(
    aes(x = name, y = row_id, fill = value, label = unseen)
  ) +
  geom_tile(alpha = 0.5, color = 'black') + 
  geom_text(color = 'black') + 
  coord_cartesian() +
  scale_y_reverse(breaks = 1:10) +
  scale_x_continuous(breaks = 1:10, position = 'top') +
  labs(
    x = 'Fold ID Number',
    y = 'Fold ID Number',
    title = '10-fold Cross Validation Example',
    caption = 'table represents all TRAINING data',
    fill = element_blank()
  ) +
  theme_minimal(base_size = 15) +
  theme(
    panel.grid = element_blank(),
    legend.position = 'bottom',
    plot.title.position = 'plot'
    ) 
```


```{r, eval = FALSE}
library(dials)
```

```{r, eval = FALSE}
cv_obj <- vfold_cv(sea_training, v = 10)
```

```{r}
max_entropy_grid <-
  grid_max_entropy(
    mtry(range = c(min(mtry_values), max(mtry_values))),
    trees(range = c(min(tree_sizes), max(tree_sizes))),
    size = 150
  )
```


```{r}
max_entropy_grid %>% 
  mutate(
    Approach = 'Maximum Entropy Grid'
  ) %>% 
  bind_rows(
    all_params %>% 
      mutate(Approach = 'Full Grid') %>% 
      rename(trees = num.tree)
  ) %>% 
  ggplot(
    aes(
      x = mtry, 
      y = trees, 
      fill = Approach,
      color = Approach,
      shape = Approach
    )
  ) +
  geom_point(size = 5, alpha = 0.5) +
  labs(
    y = 'Number of Trees',
    title = 'Coverage of Hyperparmater Space by Approach',
    subtitle = 
      'Maximum entropy grid can cover similar amount of space\nwith fewer models'
  ) +
  theme_bw(base_size = 18) +
  theme(
    legend.position = 'bottom',
    plot.title.position = 'plot'
    )
```


```{r, eval = FALSE}
tuned_res <-
  cv_it_complex(
    cv_obj = cv_obj,
    outcome_string = 'outcome_next_year',
    seed = 333,
    mod_formula = outcome_next_year ~ .,
    tuning_grid = max_entropy_grid,
    model_type = 'rf',
    mode = 'regression'
    ) %>% 
   arrange(rmse_mean)
#dir.create(here::here('data/long_runtime_objects')
write_rds(tuned_res, here::here('data/long_runtime_objects', 'day3_tuned_results.Rds'))
```

```{r}
tuned_res <- read_rds(here::here('data/long_runtime_objects', 'day3_tuned_results.Rds'))
tuned_rf <- 
  ranger(
    formula = outcome_next_year ~ ., 
    data = sea_training, 
    mtry = tuned_res$mtry[[1]],
    num.trees = tuned_res$trees[[1]],
    importance = 'permutation'
  )
```



```{r}
library(vip)
vip(tuned_rf)
```




```{r include=TRUE, echo=FALSE}
important_variables <- 
  vi(tuned_rf) %>% 
  arrange(desc(Importance)) %>% 
  slice(1:8) %>% 
  mutate(order = 1:n()) %>% 
  select(janitor_version = Variable, order)

pstr <- 
  key %>% 
  filter(janitor_version == outcome_next_year) %>% 
  pull(indicator_name)

key %>% 
  inner_join(important_variables) %>% 
  arrange(order) %>% 
  select(order, indicator_name) %>% 
  kbl() %>% 
  add_footnote(
    paste0('Outcome is ', pstr, ' in the upcoming year')
  )
```

---

```{r}
library(vip)
vip(tuned_rf)
```


```{r eval=FALSE, include=FALSE, echo=FALSE}
library(pdp)

combos  <- 
  list(
    first_second = important_variables$janitor_version[c(1, 2)],
    first_third = important_variables$janitor_version[c(1, 3)],
    second_third = important_variables$janitor_version[c(2, 3)]
    )


pds <- 
  map(
    .x = combos, 
    .f = 
      ~partial(
        tuned_rf, 
        pred.var = .x,
        progress = TRUE
      ) 
  )
  


pdps <-
  map2(
    .x = pds, 
    .y = combos,
    .f = ~.x %>% 
      ggplot(
        aes(
          x = !!sym(.y[1]),
          y = yhat,
          size = !!sym(.y[2]),
        )
      ) +
      labs(
        y = paste0('Predicted: ', pstr, ' next year'), 
        title = paste0('PDP of important variables in predicting\n', pstr, ' next year')
      ) +
      geom_point(alpha = 0.5) +
      coord_cartesian() +
      theme_bw()
  )


map2(
  .x = pdps, 
  .y = names(pdps),
  ~ggsave(
    filename = 
      here::here('imgs', paste0('day_3_pdp_', .y,'_important_vars.png')),
    plot = .x,
  )
)
```


```{r eval=FALSE, include=FALSE, echo=FALSE}
name_and_year <- 
  partial(
    tuned_rf, 
    pred.var = c('country_name', 'year'),
    progress = TRUE
  ) 

name_and_year_plt <- 
  name_and_year %>% 
  ggplot(
    aes(
      x = year, 
      y = yhat,
      fill = country_name,
      color = country_name
    )
  ) +
  geom_jitter(alpha = 0.5, height = 0.0002, width = 0.2) +
  coord_cartesian() +
  geom_smooth(se = FALSE) +
  theme_bw() + 
  labs(
    fill = 'Country' ,
    color = 'Country',
    y = paste0('Predicted: ', pstr, ' next year'),
    title = paste0('Predicted: ', pstr, ' over time'),
    caption = 'Points jittered minimally & made transparent to show overlaps'
  ) +
  theme(
    plot.title.position = 'plot',
    plot.caption.position = 'plot'
  )

ggsave(
  filename = here::here('imgs', 'day_3_pdp_name_year.png'),
  plot = name_and_year_plt,
  )
```

---

# PDP: First and Second VI Scores

```{r echo = FALSE, include = TRUE}
knitr::include_graphics(
  here::here('imgs', 'day_3_pdp_first_second_important_vars.png'),
)
```

---

# PDP: First and Third VI Scores

```{r echo = FALSE, include = TRUE}
knitr::include_graphics(
  here::here('imgs', 'day_3_pdp_first_third_important_vars.png'),
)
```

---

# PDP: Second Third VI Scores

```{r echo = FALSE, include = TRUE}
knitr::include_graphics(
  here::here('imgs', 'day_3_pdp_second_third_important_vars.png'),
)
```

---

# PDP: All VI scores

```{r eval=FALSE, include=FALSE, echo=FALSE}
top3 <- 
  partial(
    tuned_rf, 
    pred.var = important_variables$janitor_version[1:3],
    progress = TRUE
  )
```

```{r eval=FALSE, include=FALSE, echo=FALSE}
top3_plt <- 
  top3 %>% 
  ggplot(
    aes(
      x = !!sym(important_variables$janitor_version[1]), 
      y = yhat,
      fill = !!sym(important_variables$janitor_version[2]),
      color = !!sym(important_variables$janitor_version[2]),
      size = !!sym(important_variables$janitor_version[3])
    )
  ) +
  geom_point(alpha = 0.1) +
  coord_cartesian() +
 # geom_smooth(se = FALSE, alpha = 0.5 ) +
  theme_bw() + 
  labs(
    # fill = 'Country' ,
    # color = 'Country',
    #y = paste0('Predicted: ', pstr, ' next year'),
    title = paste0('Predicted: ', pstr, ' over time'),
    #caption = 'Points jittered minimally & made transparent to show overlaps'
  ) +
  theme(
    plot.title.position = 'plot',
    plot.caption.position = 'plot'
  )

ggsave(
  filename = here::here('imgs', 'day_3_pdp_top3_important_vars.png'),
  plot = top3_plt,
  )
```


```{r echo = FALSE, include = TRUE}
knitr::include_graphics(
  here::here('imgs', 'day_3_pdp_top3_important_vars.png'),
)
```


---

# PDP: By country & year

We see here that our fitted RF does not actually have different predictions by country. If we had a deeper tree, or if it were more important, it would

```{r echo = FALSE, include = TRUE}
knitr::include_graphics(
 here::here('imgs', 'day_3_pdp_name_year.png')
)
```


---

# So, what do we know about the outcome?

Well, `r pstr` appears to increase along with each of the variables we investigated. 



---

# DTs, GLMs, and PDPs (oh my)

```{r include = TRUE, echo = FALSE, eval = FALSE}
glm_pdp <- 
  partial(
    glm_comparison, 
    pred.var = c('sex', 'age', 'pclass'),
    type = 'classification'
    #which.class = 'survived'
    ) %>% 
  mutate(
    model = 'glm'
  )
```


.left-column[

These models predict survival differently

refer to: 

linear vs. piecewise age effect

constant effects across passenger class vs. differential effects of (`sex` x `class` x `age`)

]

.right-column[
```{r include = TRUE, echo = FALSE, eval = FALSE}
three_variable_pdp %>% 
  mutate(
    model = 'DT'
  ) %>% 
  bind_rows(glm_pdp) %>% 
  ggplot(
    aes(
      y = yhat, 
      x = age,
      shape = sex,
      color = model
    )
  ) + 
  theme_bw() +
  geom_point(
    size = 4, 
    alpha = 0.4
  ) +
  theme_bw(base_size = 15) +
  labs(
    x = 'Passenger Age', 
    y = 'Predicted Probability of Surviving', 
    title = 'Women & younger men (>= 15) display a substantially\n higher probability of survival',
    fill = 'Passenger\nSex'
    ) +
  theme(
    plot.title.position = 'plot',
    legend.position = 'bottom'
    ) +
  facet_wrap(vars(pclass), ncol = 1) 
```
]
---