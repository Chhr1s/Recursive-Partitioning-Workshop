<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>slides day 3</title>
    <meta charset="utf-8" />
    <meta name="author" content="Christopher M. Loan, MS" />
    <meta name="date" content="2022-02-22" />
    <script src="day_3_slides_files/header-attrs-2.16/header-attrs.js"></script>
    <link href="day_3_slides_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="day_3_slides_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <script src="day_3_slides_files/kePrint-0.0.1/kePrint.js"></script>
    <link href="day_3_slides_files/lightable-0.0.1/lightable.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# slides day 3
]
.author[
### Christopher M. Loan, MS
]
.date[
### February 22, 2022
]

---





# Workshop Progress

On Day 1, we discussed:
  * what DTs are
  * how to fit DTs
  * how to interprate/visualize DTs
  * how to evaluate DTs

On Day 2, we discussed
  
  * what RFs are
  * how to fit RFs
  * how to evaluate RFs
  * hyperparameters
  * how hyperparameters are used to improve models
  
---

# Looking Ahead

Today, we will discuss:

* Grid Search for Tuning Hyperparameters
* Cross validation
* Interpretation of Random Forests
* More Advanced Visualizations (Partial Dependency Plots)

---

# set up


```r
outcome_next_year &lt;- 'se_prm_enrr'
library(tidyverse)
```

```
## â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.2 â”€â”€
## âœ” ggplot2 3.3.6      âœ” purrr   0.3.4 
## âœ” tibble  3.1.8      âœ” dplyr   1.0.10
## âœ” tidyr   1.2.0      âœ” stringr 1.4.1 
## âœ” readr   2.1.1      âœ” forcats 0.5.1 
## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€
## âœ– dplyr::filter() masks stats::filter()
## âœ– dplyr::lag()    masks stats::lag()
```

```r
library(ranger)
library(rsample)
library(dials)
```

```
## Loading required package: scales
## 
## Attaching package: 'scales'
## 
## The following object is masked from 'package:purrr':
## 
##     discard
## 
## The following object is masked from 'package:readr':
## 
##     col_factor
```

```r
library(kableExtra)
```

```
## 
## Attaching package: 'kableExtra'
## 
## The following object is masked from 'package:dplyr':
## 
##     group_rows
```

```r
library(emojifont)
source(here::here('scripts/functions.R'))
```

---

# set up

Let's go ahead and get things set up like yesterday:



```r
key &lt;- 
  read.csv(here::here('data/keys', 'key_impute.csv'))

dat &lt;- 
  read.csv(
    here::here(
    'data', 
    'se_asia_imputed_world_bank.csv'
    ))


set.seed(022023)
sea_split &lt;- initial_split(dat)
sea_training &lt;- training(sea_split) 
sea_testing &lt;- testing(sea_split) 
```



Outcome =  &lt;u&gt;next year's&lt;/u&gt; School enrollment, primary (% gross)

Units = (% gross)

---

# let's establish two "benchmarks" to beat

## Benchmark 1: guess the average

Guessing the mean or median has common-sense validity.

The most common-sense approach is to guess the average (mean or median) `se_prm_enrr` this year to be next year's value.

If a model does not have a better RMSE than this, it is not worth using.


```r
naive_guess &lt;- median(sea_training$se_prm_enrr)
naive_guess
```

```
## [1] 105.7089
```

```r
View(key)
```

---


```r
dat %&gt;% 
  filter(country_name != 'Brunei Darussalam') %&gt;% 
  ggplot(
    aes(
      y = se_prm_enrl,
      x = year,
      fill = country_name
    )
  ) +
  geom_point() +
  facet_wrap(
    vars(country_name)
    )
```

![](day_3_slides_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

```r
key %&gt;% filter(janitor_version == outcome_next_year)
```

```
##   janitor_version                       indicator_name indicator_code import_file_source
## 1     se_prm_enrr School enrollment, primary (% gross)    SE.PRM.ENRR          education
```


---
# Benchmark 2: Defaults


```r
rf_default &lt;- 
  ranger(
    outcome_next_year ~ ., 
    data = sea_training,
    importance = 'permutation'
  )
```

---

## A Note on RMSE

The nice thing about RMSE is that it is in the units of the outcome. 

RMSE is the average error in the units of the outcome. 

$$

RMSE = 
\sqrt{

\frac{\Sigma (y_{actual} - y_{predicted})^2}{N}

}

$$

For us, this is (% gross)


```r
key %&gt;% filter(janitor_version == 'se_prm_enrr')
```

```
##   janitor_version                       indicator_name indicator_code import_file_source
## 1     se_prm_enrr School enrollment, primary (% gross)    SE.PRM.ENRR          education
```

---

# Benchmark RMSEs


```r
benchmark_rmse &lt;- 
  sea_testing %&gt;% 
  transmute(
    outcome_next_year,
    prediction_median = median(sea_training$se_prm_enrr),
    prediction_default.rf = predict(rf_default, .)$prediction
    ) %&gt;% 
  summarize(
    rmse_guess = rmse(observed_y = outcome_next_year, predicted_y = prediction_median),
    rmse_default.rf = rmse(observed_y = outcome_next_year, predicted_y = prediction_default.rf),
  )
kbl(benchmark_rmse)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; rmse_guess &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; rmse_default.rf &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 630.8253 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5047169 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

A model worth anything will be better than RMSE = 630.83
A decent model will be better than RMSE = 0.5

---

class: center, middle

# Onward &amp; Upward

Now that we've established these benchmarks, let's talk hyperparameters again

---

# Selecting Hyperparameters

Some have called tuning &amp; hyperparameter selection as much of an art as a science.

This is likely true when you're applying the basic methods we're going to cover today. 

Some automated methods are much more systematic:

* particle swarm optimization (PSO)
* genetic algorithm (GA)

Despite being more systematic:

* such methods require deeper knowledge
* most still require human decision-points
* some have their own hyper-hyperparameters!

**This is an entire field; we are only going to discuss grid search today**

---

# Grid Search

Purpose:
* (technical) Calibrate the underlying algorithms to your data
* (practical) Have the most accurate predictions 

Steps:
* make a grid of possible hyperparameters
* train models using those constraints
* estimate accuracy - MSE / classification accuracy / etc. - in predicting the outcome with unseen data for each model 

___

## Wait ... does this sound familiar?

---

# Refresher

When I demonstrated what hyperparameters are, I actually conducted 2 grid searches:
  * `num.trees`
  * `mtry`

This did not include a grid search of all possible values

---

# Let's make a grid


```r
tree_sizes &lt;- 
  seq(from = 500, to = 4000, by = 300)

mtry_values &lt;- 
  seq(from = 5, to = ncol(sea_testing)-1, 30)

all_params &lt;- 
  expand_grid(num.tree = tree_sizes, mtry = mtry_values)

dim(all_params)
```

```
## [1] 24  2
```

---

# loop through







&lt;img src="../imgs/day_3_OOB_heatmap.png" width="4872" /&gt;

---

# Predictions are pretty flat... 

As you can see, the best value isn't that much better than the worst.


On day 1, what did I say were the 3 ways to improve a model?

1. a model
2. user-specified settings (**hyperparameters**)
3. data

Because hyperparameters are evaluated on data, we actually can only find optimal settings **with respect to the training and testing data**.

By random chance, what if the training and testing sets were biased towards different settings?

This could lead to an algorithm to perform poorly, even though we conducted hyperparameter tuning.

---

# other issues with tuning via grid search

Before turning our intention to improving the data used to train, we should talk about two other issues with grid search: 

* local minimums, unless
  * you cover the **entire** space 
  * you get lucky ğŸ€

* computation time increases with number of
  * hyperparameters
  * possible hyperparameter values

**Spoiler:** we're not going to cover local minimums in this workshop. Consider independent research at a later date.

---

# Increasing the variation in predictions

k-fold (a.k.a v-fold) cross-validation CV:

* improves the issues with flat predictions.
* makes of our training data to both train and test the model.
* allows variance around fit to be assessed (e.g., standard error of OOB error)
* is done **after we withhold final testing data**

The logic is a mixture of resampling and withholding for training and testing (done before):

* the data withheld here is called the "fold testing data" or "assessment data"
* the data used to train the model is called "fold training" or "analysis data"

---

# k-fold Cross Validation

Process: 
* split the training data into "k" pieces.
* train a model with a proportion of the data ($n_{train} = \frac{k-1}{k}*N_{total}$)
* assess prediction accuracy with withheld ($n_{assess} = \frac{1}{k}*N_{total}$)

For example:`n` = 1000 *observations* &amp; `k` = 10 k-fold CV
* Each fold has `n`/10 = 1000/10 = 100 observations
* Model fit for a given hyperparameter is determined

---

# Diagram


![](day_3_slides_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;



```r
library(dials)
```


```r
cv_obj &lt;- vfold_cv(sea_training, v = 10)
```


```r
max_entropy_grid &lt;-
  grid_max_entropy(
    # default = floor(sqrt(ncol(training_data))) = 24
    mtry(range = c(5, 25)),
    trees(range = c(400, 1000)),
    size = 10
  )

tuned_res &lt;-
  cv_it_complex(
    cv_obj = cv_obj,
    outcome_string = 'outcome_next_year',
    seed = 333,
    mod_formula = outcome_next_year ~ .,
    tuning_grid = max_entropy_grid,
    model_type = 'rf',
    mode = 'regression'
    )
```


---


# DTs, GLMs, and PDPs (oh my)




.left-column[

These models predict survival differently

refer to: 

linear vs. piecewise age effect

constant effects across passenger class vs. differential effects of (`sex` x `class` x `age`)

]

.right-column[

]
---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create();
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
