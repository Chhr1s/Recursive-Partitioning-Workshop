<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Day 1 Recursive Partitioning &amp; Decision Trees</title>
    <meta charset="utf-8" />
    <meta name="author" content="Christopher M. Loan, MS" />
    <meta name="date" content="2022-02-20" />
    <script src="slides_files/header-attrs-2.19/header-attrs.js"></script>
    <link href="slides_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="slides_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <script src="slides_files/kePrint-0.0.1/kePrint.js"></script>
    <link href="slides_files/lightable-0.0.1/lightable.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Day 1
Recursive Partitioning &amp; Decision Trees
]
.author[
### Christopher M. Loan, MS
]
.date[
### February 20, 2022
]

---



&lt;style type="text/css"&gt;
.cl_transparent {
  position: relative;
  z-index: 1;
}

.cl_transparent::before {    
      content: "";
      background-image: url('imgs/img.jpg');
      background-size: cover;
      position: absolute;
      top: 0px;
      right: 0px;
      bottom: 0px;
      left: 0px;
      opacity: 0.5;
      z-index: -1;
}
&lt;/style&gt;




# Day 1 Agenda

* 5 Minutes — Introduction 
* 30 minutes — Terminology and Interpretation
* 30-60 Minutes — Theory of Recursive Partitioning for Classification and Regression (with examples)
* 15 Minutes —	Coffee Break
* 60 Minutes — Lab exercise: Fitting Decision Trees in R &amp; Model Evaluation
* 60 Minutes —	Use-Cases, Strengths, &amp; Weaknesses of Decision Trees
* 60 Minutes — Extensions of Decision Trees; Focus on Random Forests 

---

class: left, top, cl_transparent

# Introduction

.pull-left[


&lt;div style="border:solid; background-color:#b3e0ff
; opacity:0.7"&gt;

.left[

  **Currently**: 
  * **PhD Candidate**
    * *University of Oregon*
    * Quantitative Research Methods in Education
    * Dissertation extends model-based recursive partitioning
  
  * **Head Methodologist &amp; Owner**
    * *Parameter Analytics, LLC.*
    * Statistics, research design, and data science consulting firm


]

]

&lt;/div&gt;

.pull-right[

&lt;div style="border:solid; background-color:#b3e0ff
; opacity:0.7"&gt;

.left[


**Primary Education &amp; Work History**: 
  * Coding on-and-off since 2010 &amp; (near) daily since 2017
  
  * Bachelor's Degree in Neuroscience
  
  * Former high school teacher
  
  * Earned my Master's Degree in Prevention Science
      * Physical and psychological well-being
      * Public Health &amp; big data approaches 
      
  * Data Science Mentor at RStudio (now "Posit") 
]

]
&lt;/div&gt;

---

class: center, middle

# Onward to Content

---
# A Recommended Reference

.left[

**Strobl et al. (2009)**

* offer a great review of the material in this workshop
* has been cited &gt;2,300 times (according to Google Scholar)
* technical, but directed towards applied researchers and practitioners

] 

![](imgs/main_article.png)

---
class: center

# An Essential Consideration in Applied Computational Research 

.left[
* Software packages come with default settings

* Defaults are **rarely** the most robust method, the most flexible approach, etc. 

* In Machine Learning (ML) specifically, calibrating these settings is known as "hyper parameter tuning", "hyper parameter optimization", etc.

* People spend entire careers using conceptual mathematics and computer science to master this tuning/optimization

* Many non-ML pieces of programming have defaults, too

**You always need to read the software documentation when you use something**

This lecture will build this skill in targeted places, but it **intentionally goes with defaults due to time constraints**.

]

---
# Recursive Partitioning



**Recursive** — Successive or Repetitive

**Partitioning** — Splitting or Dividing

___

* Binary splits are almost always used 

* This works because repeated binary splits can approximate any functional form based on observed data (i.e., Y to X relationship)

* Binary splits are intuitive — can be interpreted as "yes/no" questions

* Called "ML" because the algorithm "learns" from the data. 

* It is "supervised" the algorithm can only learn what is labeled in the data.

* Basis of decision trees, random forests, and other Machine Learning (ML) methods

---
class: center, middle

# What fields subsume recursive partitioning? 

![](imgs/AI_ML_RP.png)
.left[
We will **not** cover (1) Unsupervised ML, e.g., k-means clustering, or (2) Other aspects of artificial intelligence, e.g., computer vision
]
---

# Decision Trees

Decision Trees (DTs) are a straightforward application of recursive partitioning.
___

**Overarching concept (non-technical)**

*Make subgroups of the observed variables which are similar in the outcome*
___

**Overarching concept (technical)**

*Use training data to identify splitting rules which optimally divide features into non-overlapping regions, where the cases within each region are similar with respect to the outcome*

---

# Decision Trees

They have many benefits: 

* Make no assumptions about distribution of data (non-parametric)

* Can identify compounding effects from observed data, even if unspecified, e.g.,
  * non-linear effects (the variable compounding on itself)
  * interactions (the variable compounding on another)

* Easy to interpret
    * Are not an "ML Black Box", a complaint with more complex models
    * Understanding DTs makes understanding complex extensions (e.g., bagging, boosting, random forests) easier

---
class: middle, center

# What is the **bare-minimum** process to fitting a decision tree?

(i.e., saying *"it's okay"* to use defaults for everything)

---
# Fitting a Decision Tree

![](imgs/training_tree.png)

Need &lt;u&gt;"features" / predictor variables&lt;/u&gt; &amp; &lt;u&gt; **corresponding "response" / outcome** &lt;/u&gt;

---
# Designed for Prediction 

![](imgs/predicting_tree.png)



---
# Understanding Predictions

Visual structure makes understanding predictions intuitive

![](imgs/inspecting_tree.png)
---
# Some Terminology

In this software (`rpart`) "yes" to the splitting rule is on the left &amp; "no" is right

![](imgs/labeled_img.png)

---
## Let's look at an example: 

`?rpart.plot::ptitanic`

Outcome / response variable
* dichotomous (survived vs. died) 
* this DT is a **classification tree**

![](imgs/titanic_help.png)
---

# Setting up

We need a package for actually fitting the data.


```r
# install.packages('rpart')
library(rpart)
```

This extension to `rpart` allows simple plotting, and we'll use a built-in data set from this package for our first example (data set = `ptitanic`)



```r
# install.packages('rpart.plot')
library(rpart.plot)
```

`rsample` is a relatively minimal, but useful package to set up the data.


```r
# install.packages('rsample')
library(rsample)
```

---
# Setting up


```r
ptitanic &lt;- 
  ptitanic %&gt;% 
  mutate(
    survived = relevel(survived, ref = 'died')
  )
split_data &lt;- initial_split(ptitanic)
training_data &lt;- training(split_data)
testing_data &lt;- testing(split_data)
```

We will start with 2 variables to predict `survival`: `age` &amp; `sex`



___

### Some descriptives 

The percent of the sample which survived was ~38%.

Sex (from full data):
  * 843 coded as male
  * 466 coded as female

Age (from full data):
  * Mean = 29.88 years
  * Std. Dev. = 14.41 years
  

---

# Fit the decision tree

Basic syntax is: `outcome_variable ~ predictors`


```r
two_var_DT &lt;- 
  rpart(
    survived ~ sex + age, 
    data = training_data
    )
```

---



---


# Decision Tree Results

.left-column[

Each node shows
* predominant predicted class
* predicted probability of survival
* percent of total sample in this node

We can make it look a little different too

]

.right-column[
![](slides_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;
]
---

# Decision Tree Results

.pull-left[

Split order, Predicted probabilities, &amp; node size give sense of the observed training data

* `sex` was most impacted survival

* more men than women

* Men (~12%) survived at a lower rate than women (~26%)

* effect of age substantial in men (younger men more likely to live)

]

.pull-right[
![](slides_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;
]
---

# How well does it perform? 

We can see how well the trained model performs with the withheld testing data. The model has not seen this. 





Overall, the model accurately classified ~78%
___

To see this with more nuance, we can look at it as below: 

&lt;table&gt;
&lt;caption&gt;Performance of 2-variable DT&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; actual &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; predicted &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; type of prediction &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; proportion &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; survived &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; survived &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 95 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; true positive &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 29% &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; died &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; died &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 161 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; true negative &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 49% &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; survived &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; died &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 37 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; false positive &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11% &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; died &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; survived &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 35 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; false negative &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11% &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Feature Extraction &amp; Interpretation
.pull-left[


```r
library(pdp)
## purrr has a `partial` command
## let's make sure it uses the one from `pdp`
partial &lt;- pdp::partial

univariate_pdp_sex &lt;- 
  partial(
    two_var_DT, 
    pred.var = 'sex',
    type = 'classification',
    which.class = 'survived'
    )
```
]
.pull-right[

![](slides_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;
]

---

```r
univariate_pdp_age &lt;- 
  partial(
    two_var_DT, 
    pred.var = 'age',
    type = 'classification',
    which.class = 'survived'
    )
```


```r
age_plt1 &lt;- 
  univariate_pdp_age %&gt;% 
  ggplot(
    aes(x = age, y = yhat)
  ) +
  geom_point() +
  theme_bw(base_size = 15) +
  labs(
    x = 'Passenger Age', 
    y = 'Predicted Probability of Surviving', 
    title = 'Younger individuals (&gt;= 15) display a \nsubstantially higher probability of survival'
    ) +
  theme(plot.title.position = 'plot') +
  scale_y_continuous(
    limits = c(-0.5, 1),
    breaks = seq(-0.4, 1, 0.2)
    )
```

---


```r
age_plt1
```

![](slides_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;

---


```r
bivariate_pdp &lt;- 
  partial(
    two_var_DT, 
    pred.var = c('age', 'sex'),
    type = 'classification',
    which.class = 'survived'
    )
```

---


```r
bi_pdp_plt &lt;- 
  bivariate_pdp %&gt;% 
  ggplot(
    aes(
      y = yhat, 
      x = age,
      color = sex
    )
  ) + 
  theme_bw() +
  geom_point() +
  theme_bw(base_size = 15) +
  labs(
    x = 'Passenger Age', 
    y = 'Predicted Probability of Surviving', 
    title = 'Women &amp; younger men (&gt;= 15) display a substantially\n higher probability of survival',
    fill = 'Passenger\nSex'
    ) +
  theme(
    plot.title.position = 'plot'
    ) +
  scale_y_continuous(
    limits = c(-1, 1),
    breaks = seq(-1, 1, 0.25)
    )
```

---


```r
bi_pdp_plt
```

![](slides_files/figure-html/unnamed-chunk-23-1.png)&lt;!-- --&gt;

---

# Improving model performance 

In many `R` packages, the `.` in a formula indicates that you want all variables (except the outcome) included


```r
all_var_DT &lt;- 
  rpart(
    survived ~ ., 
    data = training_data
    )
```

---



```r
# I typically do NOT recommend this
three_variable_pdp &lt;- 
  partial(
    all_var_DT, 
    pred.var = c('age', 'sex', 'pclass'),
    type = 'classification',
    which.class = 'survived'
    )
```


---


```r
#three_variable_plt &lt;- 
  three_variable_pdp %&gt;% 
  ggplot(
    aes(
      y = yhat, 
      x = age,
      shape = pclass,
      color = pclass
    )
  ) + 
  theme_bw() +
  geom_point(
    size = 4, 
    alpha = 0.4
  ) +
  theme_bw(base_size = 15) +
  labs(
    x = 'Passenger Age', 
    y = 'Predicted Probability of Surviving', 
    title = 'Women &amp; younger men (&gt;= 15) display a substantially\n higher probability of survival',
    fill = 'Passenger\nSex'
    ) +
  theme(
    plot.title.position = 'plot'
    ) +
  facet_wrap(vars(sex), ncol = 2)
```

![](slides_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;

---


```r
three_variable_plt
```

![](slides_files/figure-html/unnamed-chunk-27-1.png)&lt;!-- --&gt;

---


```r
glm_comparison &lt;-
  glm(
    data = training_data,
    formula = survived ~ ., 
    family = binomial()
  )
```



---

Let's contrast this to a logistic regression




# Benefits of decision trees

* work for many outcome types
    * dichotomous = "classification" tree
    * continuous = "regression" tree
    
    
    
---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create();
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
