---
title: "slides day 2"
author: "Christopher M. Loan, MS"
date: 'February 21, 2022'
output: xaringan::moon_reader
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
outcome_next_year <- 'se_xpd_totl_gd_zs'
```

# Set up 

```{r}
set.seed(022023)
library(tidyverse)
library(ranger)
library(rsample)
library(vip)
library(kableExtra)
library(missRanger)
library(emojifont)
library(dials)
source(here::here('scripts', 'functions.R'))
ptitanic <- rpart.plot::ptitanic
```

---

# Load Data


There are ways to handle missing data with random forests
* `ranger` does not embed them
* [`missRanger`](https://cran.r-project.org/web/packages/missRanger/vignettes/missRanger.html) (built on `ranger` actually), `mice`, and `Ameila` can all be used to impute data
* this is beyond the scope of the workshop


```{r}
ptitanic <- 
  ptitanic %>% 
  mutate(
    survived = relevel(survived, ref = 'died')
  ) %>% 
  drop_na() %>% 
  missRanger(pmm.k=1, verbose = FALSE)

split_data <- initial_split(ptitanic)
training_data <- training(split_data) 
testing_data <- testing(split_data)
```

---

class: center, middle

# Random Forests

---

# Underlying Logic

Random forests (RFs) are an **ensemble** method designed to improve several shortcomings of decision trees.

The logic is relatively simple: 
  * Create a bootstrapped data set
  * Fit a decision tree with a random subset of variables at each split point (often called `mtry`)
  * Repeat 500+ times (often called `ntree`/`num_tree`, etc.)

---

# Predictions
  
Prediction with RFs is just weighted DTs
* Each (e.g.) 500 trees predict the outcome with new data
* Typically, each tree gets a weight of 1/`ntree` 


.center[
$prediction_{total} = \Sigma(\frac{1}{ntree} * prediction_{indiviual})$
]

---

# Connecting to Decision Trees

<u>**B**</u>ootstrap <u>**AG**</u>gregated predictions are called <u>**bag**</u>ged predictions

If the RFs could select from all variables, RFs would be bootstrap aggregated ("bagged") decision trees.

* Resampling is done with replacement, which results in some non-selected cases
* These cases are called the out-of-bag (OOB) sample
* The OOB sample can be used to test prediction accuracy
* OOB Error can thus be a measure of accuracy / error, and can be inspected similarly to residuals

**Let's look at some diagrams to clear things up**

---
class: center, middle

# Bootstrapping

```{r echo=FALSE, include=TRUE}
knitr::include_graphics(
  here::here(
    'imgs/bootstraping.png'
  )
)
```


---

class: center, middle

# Subsetting Predictors (at each split)

```{r echo=FALSE, include=TRUE}
#![](imgs/subsetting_predictors.png)
knitr::include_graphics(
  here::here(
    'imgs/subsetting_predictors.png'
  )
  )
```

---

# The Value of Variation

RFs use the aggregate prediction from many, (potentially) different trees

Trees which comprise the RF can vary greatly compared to a traditional DT on the same sample 

RFs achieve this by
  * Using a bootstrapped sample for each tree
  * Leveraging only a subset of variables at each DT's splits
  * Making prediciton with a "voting" system 
    * each tree predicts unseen data
    * each prediction is a "vote" (i.e., a weighted prediction) 
    * sum of weights used to calculate the final prediction
  
**In essence, many *similar yet distinct* decision trees work better together than a single decision tree**

.center[

`r emoji('evergreen_tree')` = `r emoji('slightly_smiling_face')` 

$\Sigma$ `r sample(emoji(search_emoji('tree')), 10, replace = TRUE)`... (500th tree) = `r emoji('smiley')`

]
---

# Benefits of Random Forests

* when number of predictors > number of observed units
  * established protocols for variable importance 
  * not subject to ordering effects like other algorithmic approaches (e.g., step-wise regression)
  
---

class: middle, center

# What is the **bare-minimum** process to fitting a random forest?

(i.e., saying *"it's okay"* to use defaults for everything)

---

```{r}
rf_1 <- 
 ranger(
    formula = survived ~ ., 
    data = training_data,
    importance = 'impurity',
    probability = TRUE
    )
```

---

# Variable Importance Plot

.left-column[
The VIP from the RF is very similar to the VIP from the DT
]

.right-column[
```{r echo = FALSE, include = TRUE}
vip(rf_1) + theme_bw(base_size = 18) + 
  labs(y = 'Importance (Gini Impurity Index)') +
  aes(fill = 1, alpha = 0.7) +
  theme(legend.position = 'none')
```
]

---

# Accuracy


```{r echo = FALSE, include = TRUE}
tmp_plt_df <- 
  testing_data %>% 
  mutate(
    predicted = 
       predict(rf_1, data = .)$predictions[,'survived'],
    actual = survived,
    predicted_class = 
      if_else(round(predicted) == 1, 'survived', 'died'),
    accuracy = 
      if_else(predicted_class == actual, 'correct', 'incorrect')
  )

plt_df <- 
  tmp_plt_df %>% 
  count(accuracy) %>% 
  mutate(
    proportion = 
      paste0(
        round(
          (n / sum(n))* 100), 
        '%'
        )
    )
```

```{r echo = FALSE, include = TRUE}
plt_df %>% kbl(caption = 'Model Performance (no imputation)')
```

# In-Depth Accuracy

```{r echo = FALSE, include=TRUE}
tmp_plt_df %>%
  mutate(
   type_of_prediction =
     case_when(
       actual == 'died' & predicted_class == 'died' ~ 
         'true negative',
       actual == 'died' & predicted_class == 'survived' ~ 
         'false negative',
       actual == 'survived' & predicted_class == 'died' ~ 
         'false positive',
       actual == 'survived' & predicted_class == 'survived' ~ 
         'true positive'
     )
  ) %>%
  count(type_of_prediction) %>% 
  rename(`Type of Accuracy` = type_of_prediction) %>% 
  mutate(
    percent = paste0(round(100*n/sum(n)), '%')
  ) %>% 
  ungroup() %>% 
  kbl(caption = 'Model Performance by Type of Accuracy (no imputation)')
```


---
class: center, middle

# Now it's your turn

---

class: center, middle

# Day 2 (Part 2): Model Improvement
---

# Hyperparameters

hyperparameters = <u>Analyst-specified</u> parameters

* Specified by analyst
* These parameters can be thought of as the "settings" of your model 
* Different settings are better for different circumstances

Hyperparameters influence <u>model-estimated</u> parameters

* Learned from the data
* *Conceptually* similar to regression coefficients


___

The process of modifying these is called hyperparameter "tuning"

"Hyperparameter optimization (HPO)" may be used interchangeably with tuning, though some take this to imply automated tuning procedures

We'll start with manual tuning

---

# Hyperparameters 

Another analogy comes from calibrating any machine.

For example, consider a camera `r emoji('camera')`

* You can change the lens, film, aperture, focal distance etc., (hyperparameters)
* The camera will allow light onto the film and store the emergent pattern (parameters)
* Clear pictures do not always occur with the same settings
* Different input data (scale/lighting/etc.) are best captured with different settings
* You can use input data to automatically tune hyperparameters
    * Same concept as "auto-focusing" cameras
    * Beyond the scope of this workshop

---

# Hyperparameters: Random Forests

We have discussed 2 hyperparameters for random forests:

* `mtry` - the number of variables to choose from at each split
* `ntree` - the number of decision trees to grow
* there are many others though
* a few important hyperparameters are:
     * `importance` 
     * `max.depth`
     * `min.node.size`

* see `?ranger` to see additional


---

# Hyperparameters: Objective Functions 

The process of choosing the best hyperparameters is always the same

**Set an objective function — some form of prediction error — and find the minimum**


Like always, we need to check the default of what `ranger()` provides:

By default `prediction.error` outputs 
  * mean squared error (MSE; for regression)
  * fraction of missclassified samples (for classification)
  

.center[
$MSE = \frac{1}{n}\Sigma(Y_{observed} - Y_{predicted})^2$
]

The choice of objective function is too deep of a dive for this workshop

Some circumstances necessitate different objective functions based on what you care about

---

# Manually finding the minimum

There are complex ways to find the best minimum, we will talk about some tomorrow

For now, let's
  
  * create a list of possible hyperparameter values for `num.trees`
  * fit a random forest with each of those values
  * select the value with the best OOB prediction error
  * repeat for `mtry`

---

# Real-World Data

We've outgrown the titanic example at this point. 

There are so few variables available, making demonstrating tuning less meaningful.

Let's load the WB South East Asia Data.

```{r}
key <- 
  read.csv(here::here('data/keys', 'key_impute.csv'))

dat <- 
  read.csv(
    here::here(
    'data', 
    'se_asia_imputed_world_bank.csv'
    ))

sea_split <- initial_split(dat)
sea_training <- training(sea_split) 
sea_testing <- testing(sea_split) 
```


---

# Varying `num.trees`

First we make a vector of the `tree_sizes` we want to use:

```{r}
tree_sizes <- 
  seq(from = 300, to = 5000, by = 100)
```

use a loop to fit `ranger()` to all values of `num.trees`

```{r}
rfs_num_tree <- vector('list', length(tree_sizes))

for (i in 1:length(tree_sizes)){
  rfs_num_tree[[i]] <- 
    ranger(
      outcome_next_year ~ ., 
      data = sea_training,
      num.trees = tree_sizes[[i]]
    )
  #print(paste0('iteration #', i, ' complete'))
}
```

---

# Varying `num.trees`

```{r eval=TRUE, include=FALSE}
names(rfs_num_tree) <- paste0('num.trees = ', tree_sizes)

pred_errors_num_tree <- 
  rfs_num_tree %>%  
  map_dbl(~.x$prediction.error)
```


```{r, echo = FALSE, include = TRUE}
num_trees_plt <- 
  data.frame(
  pred_error = pred_errors_num_tree, 
  num_trees = tree_sizes
) %>% 
  ggplot(
    aes(
      x = num_trees,
      y = pred_error
      )
  ) + 
  geom_line() +
  geom_label(
    inherit.aes = FALSE,
    label = 'Model Default\n num.trees = 500', 
    y = pred_errors_num_tree[which.max(pred_errors_num_tree)] - 0.03* pred_errors_num_tree[which.max(pred_errors_num_tree)],
    x = 1100) +
  geom_vline(
    xintercept = 500, 
    size = 1.5, 
    linetype = 2, 
    color = 'gray50'
  ) +
  geom_smooth() +
  labs(
    title = 'OOB Prediction Error by Number of Trees',
    y = 'Out of Bag (OOB) Error (Mean Square Error)'

    ) +
  theme_bw() 

num_trees_plt 
#Overall out of bag prediction error. For classification this is the fraction of missclassified samples, for probability estimation the Brier score, for regression the mean squared error and for survival one minus Harrell's C-index.

```

---

# Varying `mtry`

```{r echo = FALSE, include = TRUE}
mtry_values <- 
  seq(1, ncol(sea_testing)-1, 1)

rfs_mtry <- vector('list', length(mtry_values))

for (i in 1:length(mtry_values)){
  rfs_mtry[[i]] <- 
    ranger(
      outcome_next_year ~ ., 
      data = sea_training,
      mtry = mtry_values[[i]],
      num.threads = 8L
    )
  #print(paste0('iteration #', i, ' complete'))
}

names(rfs_mtry) <- paste0('mtry = ', mtry_values)

pred_errors_mtry <- 
  rfs_mtry %>%  
  map_dbl(~.x$prediction.error)
```


```{r echo = FALSE, include = TRUE}
mtry_plt <- 
  data.frame(
  pred_error = pred_errors_mtry, 
  mtry = mtry_values
) %>% 
  ggplot(
    aes(
      x = mtry,
      y = pred_error
      )
  ) + 
   geom_vline(
    xintercept = floor(sqrt(ncol(sea_training)-1)), 
    size = 1.5, 
    linetype = 2, 
    color = 'gray50'
  ) +
  geom_label(
    inherit.aes = FALSE,
    label =
      paste0(
        'Model Default\nmtry = ', floor(sqrt(ncol(sea_training)-1))
      ), 
    y = pred_errors_mtry[which.max(pred_errors_mtry)] - 0.1* pred_errors_mtry[which.max(pred_errors_mtry)],
    x = floor(
      sqrt(
        ncol(sea_training)-1) #+ 50
    )
  ) +
  geom_line() +
  theme_bw() +
  labs(
    title = 'OOB Prediction Error by mtry', 
    x = 'mtry',
    y = 'Out of Bag (OOB) Error (Mean Square Error)'
  )
```


```{r echo = FALSE, include = TRUE}
mtry_plt
```

---

# select smallest error

```{r}
best_num.trees <- tree_sizes[which.min(pred_errors_num_tree)]
best_num.trees

best_mtry <- mtry_values[which.min(pred_errors_mtry)]
best_mtry
```


```{r}
manually_tuned_rf <- 
  ranger(
    outcome_next_year ~ ., 
    data = sea_training,
    num.trees = best_num.trees,
    mtry = best_mtry, 
    importance = 'permutation'
  )
```

---

# Manually Tuning Overlaid (`num.trees`)

```{r echo = FALSE, include = TRUE}
num_trees_plt +
  geom_hline(
    color = 'cornflowerblue', 
    yintercept = manually_tuned_rf$prediction.error, 
    size = 2, 
    linetype = 2, 
    alpha = 0.8
    ) +
  geom_label(
    color = 'cornflowerblue', 
    x = max(tree_sizes) - max(tree_sizes)*0.25,
    y = manually_tuned_rf$prediction.error + 0.025*manually_tuned_rf$prediction.error,
    label = 'Manually Tuned\nOOB Error'
    )
```

---

# Manually Tuning Overlaid (`mtry`)

```{r echo = FALSE, include = TRUE}
mtry_plt +
  geom_hline(
    color = 'cornflowerblue', 
    yintercept = manually_tuned_rf$prediction.error,
    size = 2, 
    linetype = 2, 
    alpha = 0.8
    ) +
  geom_label(
    color = 'cornflowerblue', 
    x = max(mtry_values) - max(mtry_values)*0.25,
    y = manually_tuned_rf$prediction.error + 0.025*manually_tuned_rf$prediction.error,
    label = 'Manually Tuned\nOOB Error',
    )
```

---

# `r emoji('tired_face')` `r emoji('angry')` We spent all that time & the manually tuned model is not the best?! 

Well... yes

We found the best hyperparameter value, **assuming the other hyperparameters were constant**.

In other words, if we wanted to tune 2 hyper parameters, we'd have to do that simultaneously. 

If you're familiar with interactions in multiple regression, this concept is familiar: 

the influence of one hyperparameter on the model may differ based on values of another hyperparameter.

Tomorrow we'll tune both hyperparameters together `r emoji('smile')`

---

# Quick VIP

A deeper interpretation will have to wait until tomorrow!

```{r echo = FALSE, include = TRUE}
vip(manually_tuned_rf)
```

---

# Extract names from key

```{r include=TRUE, echo=FALSE}


important_variables <- 
  vi(tuned_rf) %>% 
  arrange(desc(Importance)) %>% 
  slice(1:8) %>% 
  mutate(order = 1:n()) %>% 
  select(janitor_version = Variable, order)

pstr <- 
  key %>% 
  filter(janitor_version == outcome_next_year) %>% 
  pull(indicator_name)

key %>% 
  inner_join(important_variables) %>% 
  arrange(order) %>% 
  select(order, indicator_name) %>% 
  kbl() %>% 
  add_footnote(
    paste0('Outcome is ', pstr, ' in the upcoming year')
  )
# 
# 
# important_variables <- 
#   vi(manually_tuned_rf) %>% 
#   arrange(desc(Importance)) %>% 
#   slice(1:8) %>% 
#   pull(Variable)
# 
# pstr <- 
#   key %>% 
#   filter(janitor_version == outcome_next_year) %>% 
#   pull(indicator_name)
# 
# key %>% 
#   filter(
#     janitor_version %in% important_variables
#   ) %>% 
#   mutate(
#     janitor_version = fct_reorder(janitor_version, important_variables),
#     importance = as.numeric(janitor_version)
#   ) %>% 
#   arrange(janitor_version) %>% 
#   select(importance, indicator_name) %>% 
#   kbl() %>% 
#   add_footnote(
#     paste0('Outcome is ', pstr, ' in the upcoming year')
#   )
```


```{r include=TRUE, echo=FALSE}
manually_tuned_rf$prediction.error
```


---

# Interpretation: Expectation

Historically, researchers have criticized RFs and other ensemble methods of being "black boxes" because
* branching diagrams (as with DTs) are easy to interpret
* RFs are 500+ DTs (on bootstrapped samples, with a subset of variables)

How would you go about disentangling effects?  

```{r echo=FALSE, include=TRUE}
knitr::include_graphics(
  here::here(
    'imgs/black_box.png'
  )
)
```

---

# Interpretation: Reality 

Keep in mind that prediction - not interpretation - was the original goal of RFs

That said:
* Several methods have been developed to interpret ensemble models
* Predicted outcomes / probabilities at various levels of covariates make models easy to interpret 


```{r echo=FALSE, include=TRUE}
knitr::include_graphics(
  here::here(
    'imgs/unboxing.png'
  )
  )
```

---

# Workshop Progress

Yesterday, we discussed:
  * what DTs are
  * how to fit DTs
  * how to interprate/visualize DTs
  * how to evaluate DTs

Today, we discussed
  
  * what RFs are
  * how to fit RFs
  * how to evaluate RFs
  * hyperparameters
  * how hyperparameters are used to improve models
  
---

# Looking Ahead

Tomorrow, we will discuss:

* Interpretation of Random Forests
* More Advanced Visualizations (Partial Dependency Plots)
* Cross validation
* Tuning More than 1 Hyperparameter at a time

