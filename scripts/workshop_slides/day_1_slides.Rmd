---
title: "Day 1\nRecursive Partitioning & Decision Trees"
author: "Christopher M. Loan, MS"
date: 'February 20, 2022'
output: xaringan::moon_reader
---

class: center, middle

```{r include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```


```{r xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

# A Recommended Reference

.left[

**Strobl et al. (2009)**

* offer a great review of the material in this workshop
* has been cited >2,300 times (according to Google Scholar)
* technical, but directed towards applied researchers and practitioners

] 

```{r include = TRUE, echo = FALSE}
knitr::include_graphics(
  here::here(
    'imgs/main_article.png'
  )
)
```


---
class: center

# An Essential Consideration in Applied Computational Research 

.left[

Software packages come with default settings

Be aware that:

* Defaults are **rarely** the best approach (robustness, flexibility, speed)

* <u>**You ALWAYS need to read the software documentation when you use something**</u>

* This lecture will build this skill in targeted places, but it **intentionally goes with defaults due to time constraints**. 

* This is very important when we "tune hyperparameters" later

]

---



# Recursive Partitioning: The focus on this workshop

```{r include=FALSE}
library(tidyverse)
```

**Recursive** — Successive or Repetitive

**Partitioning** — Splitting or Dividing

___

* Binary splits are almost always used (some "multi-way" splitting models exist)

* This works because repeated binary splits can approximate any functional form based on observed data (i.e., Y to X relationship)

* Binary splits are intuitive — can be interpreted as "yes/no" questions

___ 

**Decision Trees & Random Forests are**

* 2 of the most prominent applications of recursive partitioning
* the focus of the workshop

---
class: center, middle

# What fields subsume recursive partitioning? 
.left[
* "Machine Learning" (ML) because the algorithm "learns" from the data. 
* "supervised" because the algorithm can only learn what is **labeled** in the data (i.e., humans who make the data are "supervising")
]
```{r include = TRUE, echo = FALSE}
knitr::include_graphics(
  here::here(
    'imgs/AI_ML_RP.png'
  )
)
# .left[
# We will **not** cover (1) Unsupervised ML, e.g., k-means clustering, or (2) Other aspects of artificial intelligence, e.g., computer vision
# ]
```


---

# Decision Trees & Random Forests

Many consider the automated interaction detection proposed in 1963 to be the seminal reference in this field.

`Morgan, J. N., & Sonquist, J. A. (1963). Problems in the analysis of survey data, and a proposal. Journal of the American Statis- tical Association, 58, 415–434.`

This was (in some ways) superseded by 

`Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification and regression trees. New York: Chapman & Hall.`

The same first author later released this

`Breiman, L. (2001a). Random forests. Machine Learning, 45, 5–32.`

---

# Decision Trees

Decision Trees (DTs) are a straightforward application of recursive partitioning.
___

**Overarching concept (non-technical)**

*Make subgroups of the observed variables which are similar in the outcome*
___

**Overarching concept (technical)**

*Use training data to identify splitting rules which optimally divide features into non-overlapping regions; as determined via optimization of an objective function, these regions should ensure cases within each region are maximally similar with respect to the outcome*

---

# Decision Trees

They have many benefits: 

* Easy to interpret
* Make no assumptions about distribution of data (non-parametric)

* Can identify compounding effects from observed data, even if unspecified, e.g.,
  * non-linear effects (the variable compounding on itself)
  * interactions (the variable compounding on another)

* Understanding DTs makes understanding complex extensions (e.g., bagging, boosting, random forests) easier

* Better handle large numbers of predictors, compared to parametric models:
  * Do not need to do dimension reduction (e.g., PCA/SEM) with many variables
  * Do not "lose power" with more variables

---

class: middle, center

# What is the **bare-minimum** process to fitting a decision tree?

(i.e., saying *"it's okay"* to use defaults for everything)

---

# Fitting a Decision Tree


```{r include = TRUE, echo = FALSE}
knitr::include_graphics(
  here::here(
    'imgs/training_tree.png'
  )
)
```

Need <u>"features" / predictor variables</u> & <u> *corresponding "response" / outcome variables* </u>

---

# Designed for Prediction 

```{r include = TRUE, echo = FALSE}
knitr::include_graphics(
  here::here(
    'imgs/predicting_tree.png'
  )
)
```

---
# Understanding Predictions

Visual structure makes understanding predictions intuitive

```{r include = TRUE, echo = FALSE}
knitr::include_graphics(
  here::here(
    'imgs/inspecting_tree.png'
  )
)
```

---
# Some Terminology

In this software (`rpart`) "yes" to the splitting rule is on the left & "no" is right

```{r include = TRUE, echo = FALSE}
knitr::include_graphics(
  here::here(
    'imgs/labeled_img.png'
  )
)
```

---

## Let's look at an example: 

`?rpart.plot::ptitanic`

Outcome / response variable
* dichotomous (survived vs. died) 
* this DT is a **classification tree**

```{r include = TRUE, echo = FALSE}
knitr::include_graphics(
  here::here(
    'imgs/titanic_help.png'
  )
)
```

---

# Setting up

We need a package for actually fitting the data.

```{r}
# install.packages('rpart')
library(rpart)
```

This extension to `rpart` allows simple plotting, and we'll use a built-in data set from this package for our first example (data set = `ptitanic`)


```{r}
# install.packages('rpart.plot')
library(rpart.plot)
```

`rsample` is a relatively minimal, but useful package to set up the data.

```{r}
# install.packages('rsample')
library(rsample)
```

---
# Setting up

ADD SOME INFORMATION ON WHAT HAPPENED TO THE TITANIC

```{r}
data(ptitanic)
ptitanic <- 
  ptitanic %>% 
  mutate(
    survived = relevel(survived, ref = 'died')
  )
set.seed(022023)
# this is how we'll keep training and testing data apart
split_data <- initial_split(ptitanic)
training_data <- training(split_data)
testing_data <- testing(split_data)
```

We will start with 2 variables to predict `survival`: `age` & `sex`


---

```{r include=FALSE}
descriptives <- psych::describe(rpart.plot::ptitanic$age)
```


# Some descriptives 

In the whole sample, ~`r round(sum(ptitanic$survived == 'survived')/nrow(ptitanic)*100)`% survived the incident.

Records show:
* `r sum(ptitanic$sex == 'male')` coded as male
* `r sum(ptitanic$sex == 'female')` coded as female:
* Age - Mean = `r round(descriptives$mean, 2)` years
* Age - Std. Dev. = `r round(descriptives$sd, 2)` years

---

# Fit the decision tree

Basic syntax is: `outcome_variable ~ predictors`

Let's start with a really simple model to get the hang of DTs.

```{r}
two_var_DT <- 
  rpart(
    survived ~ sex + age, 
    data = training_data
  )
```

This asks the model to build a decision tree that could predict `survival` based on these two variables in the data (coded `sex` and `age`)

```{r include=FALSE}
surv_by_sex <- 
  training_data %>% 
  count(sex, survived) %>% 
  mutate(perc = 100* n / sum(n)) %>% 
  filter(survived == 'survived') %>% 
  select(sex, perc) %>% 
  split(., .$sex) %>% 
  map(
    ~paste0('~', round(pull(.x, perc)), '%')
  )
```

---

# Decision Tree Results

.left-column[

#### Each node shows
predominant predicted class

predicted probability of survival

percent of total sample in this node

]

.right-column[
```{r echo=TRUE, include = TRUE, fig.height=5}
rpart.plot(two_var_DT, type = 4, 
           clip.right.labs = FALSE)
```
]
---

# Decision Tree Results

.pull-left[

Split order, Predicted probabilities, & node size give sense of the observed training data

* `sex` was most impacted survival

* more men than women

* Men (`r surv_by_sex$male`) survived at a lower rate than women (`r surv_by_sex$female`)

]

.pull-right[
```{r echo=FALSE}
rpart.plot(
  two_var_DT, 
  type = 4, 
  clip.right.labs = FALSE
  )
```
]

---

# Zoomed in

```{r echo=FALSE, fig.align='center', fig.width = 10, fig.height=7}
rpart.plot(
  two_var_DT, 
  type = 4, 
  clip.right.labs = FALSE,
  cex = 1.5
  )
```

---

# Variable Importance

* Variable importance give the relative contribution of each variable to the model. 
* As complexity / size of tree increases, variable importance plots (VIPs) become more helpful after the fist few splits.
* Depending on the modeling framework, they are created with slightly different approaches.
* Interpretation of VI magnitudes depends on the measure.
* This is beyond the scope of this workshop.

___

### Variable importance (in this workshop)

* The `vip` package can calculate variable importance for all the models we'll use
* Higher importance values indicate greater importance in all examples we'll discuss
* Again, **<u>always check the help documentation</u>**


---

# Titanic VIP

Once we load the library, 

```{r, eval = FALSE, echo = T}
library(vip)
```

we can get a VIP for our decision tree in 1 line of code.

```{r, eval = FALSE, echo = T}
plt <- 
  vip(two_var_DT, include_type = TRUE)
```

If we want to label the axes or format, we treat it like `ggplot2` objects

```{r, eval = FALSE, echo = T}
plt + 
  theme_bw(base_size = 15) +
  labs(
    title = 'Variable Importance Plot for Decision Tree',
    subtitle = 'Data from `ptitanic`',
  )
```

---

# Titanic VIP

```{r echo = FALSE, fig.align='center', fig.width=12, fig.height=7}
library(vip)
vip(two_var_DT, include_type = TRUE) + 
  theme_bw(base_size = 15) +
  labs(
    title = 'Variable Importance Plot for 2-Variable Decision Tree',
    subtitle = 'Data from `ptitanic`'
  )
```

---

# Visualizing the Decision Boundaries

.left-column[

Visually, we are asking the model to draw the fewest lines possible to get maximum separation of triangles and circles.

First we separate men from women, then young men from old men.

]

.right-column[

```{r echo=FALSE, fig.align='center', fig.height = 6}
training_data %>% 
  drop_na() %>% 
  mutate(
    prediction = 
      if_else(
        round(predict(two_var_DT, newdata = .)[,'survived']) == 1, 'survived', 'died'
      ), 
    accuracy = if_else(prediction == survived, 'accurate', 'inaccurate')
  ) %>% 
  ggplot() +
  geom_jitter(
    aes(
      shape = prediction,
      x = age,
      y = sex,
      alpha = accuracy
    ),
    size = 4, 
    width = 0,
    height = 0.2
  ) +
  geom_rect(
    inherit.aes = FALSE,
    data = 
      tibble(
        y = c(1.5, 0, 1.5),
        ymax = c(3, 1.5, 3),
        x = c(0, 0, 8.5), 
        xmax = c(8.5, 80, 80),
        prediction = c('survived', 'survived', 'died')
      ),
    aes(
      xmin = x,
      xmax = xmax,
      fill = prediction,
      ymin = y, 
      ymax = ymax
    ),
    alpha = 0.4,
    color = 'black'
  )  +
  scale_alpha_discrete(range = c(0.9, 0.4)) +
  theme_bw(base_size = 15) +
  labs(
    title = 'Visualization of 2-variable Decision Tree Cutpoints',
    subtitle = 'Decision tree given 2 variables: sex and age', 
    caption = 'Data source = rpart.plot::ptitanic',
    y = element_blank()
    ) +
  theme(
    plot.title.position = 'plot', 
    plot.caption.position = 'plot'
  )
```
]
---

class: center, middle

# Evaluate Performance

---

# Evaluate Performance

We can see how well the trained model performs with the withheld testing data:

* pipe the data to mutate, and use `predict()`
* specify where the piped information needs to go 
  * `predict()` wants the model as the first argument
  * `newdata = .`

Remember, the model has not seen `testing_data`


```{r}
preds <- 
  predict(
    two_var_DT, 
    newdata = testing_data,
    type = 'class'
    )
actual <- testing_data$survived
```

---

# Predictions with Decision Trees

Let's peak at the data we made:

```{r}
table(preds, actual)
```


---

# Accuracy

```{r}
accuracy_vector <- 
  if_else(preds == actual, 'accurate', 'inaccurate')

percent_accurate <- 
  table(accuracy_vector)/length(accuracy_vector)*100

round(percent_accurate)
```

---

# In-Depth Accuracy

```{r include = FALSE}
tmp_tbl <- 
  pred_df %>% 
  count(survived, predicted) %>% 
  mutate(
    type_of_prediction = 
      case_when(
        survived == 'died' & predicted == 'died' ~ 'true negative', 
        survived == 'died' & predicted == 'survived' ~ 'false negative', 
        survived == 'survived' & predicted == 'died' ~ 'false positive',
        survived == 'survived' & predicted == 'survived' ~ 'true positive'
      ),
    percent = 
      paste0(
        round(
          (n / sum(n))* 100), 
        '%'
      )
  ) %>% 
  arrange(desc(type_of_prediction)) %>% 
  rename(
    `type of prediction` = type_of_prediction
  ) 
```

```{r include=FALSE}
tmp_describer <- 
  tmp_tbl %>% 
  mutate(sum_n = sum(n)) %>% 
  filter(survived == predicted) %>% 
  mutate(
    correct_prop = 
      paste0('~', round(sum(n)/sum_n*100), '%'), 
  ) %>% 
  distinct(correct_prop) %>% 
  pull(correct_prop)
```

Overall, the model accurately classified `r tmp_describer`
___

To see this with more nuance, we can look at it as below: 

```{r echo=FALSE}
library(kableExtra)
kbl(tmp_tbl, caption = 'Performance of 2-variable DT')
```

---

class: center, middle

# Activity

---

## Now it's your turn: 

* Open `activity_1.Rmd`.

* Complete `sections 1.1 - 1.5`.

* Work in small groups or alone.

* We'll come back together & discuss in ~20 minutes. 

---

class: center, middle

# Review `activity_1_key.Rmd`

---

# Context: DTs vs. GLMs

Let's fit the same model from the activity again. 

If training data differs from time to time, be sure you use `set.seed()`


```{r}
all_var_DT <- 
  rpart(
    survived ~ ., 
    data = training_data
  )
```

---

# Context: DTs vs. GLMs

The generalized linear model (GLM) can also make predictions. 

Essentially the same as linear regression, but for dichotomous outcomes (i.e., died vs. survived)

Let's contextualize how well this model is doing by fitting a basic logistic regression.

```{r}
glm_comparison <-
  glm(
    data = training_data,
    formula = survived ~ ., 
    family = binomial(link = "logit")
  )
```

---

# Logistic Regression Parameters

The direction of effects do not conflict with the DT. 

On day 3, we'll dive into model interpretation more and will interpret how these effects differ from the DT

```{r include = TRUE, echo = FALSE}
glm_comparison %>% 
  broom::tidy() %>% 
  mutate(
   odds_ratio = exp(coef(glm_comparison)),
   p.value = if_else(p.value < 0.001, '<0.001', paste0(round(p.value, 3)))
  ) %>% 
  kbl(caption = 'Estimated Parameters', digits = 3)
```

---

# (extra slide if time)

Exponentiate Parameters -> Odds Ratios

Intercept = Odds ratio for "full statistical reference" 

Full statistical reference is:
  * `pclass` = `1st`
  * `sex` = `female`
  * `age` = 0 (because not mean centered)
  * `sibsp` = 0
  * `parch` = 0
  
Interpreting coefficients:

From [UCLA's Statistical Methods & Data Analytics website](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/): 

*each estimated coefficient is the expected change in the log odds of [surviving] for a unit increase in the corresponding predictor variable holding the other predictor variables constant...Each exponentiated coefficient is the ratio of two odds, or the change in odds [...] for a unit increase in the corresponding predictor variable holding other variables at certain value [typically 0].*

---

```{r include = FALSE}
odds_ratios <- exp(coef(glm_comparison))
f1 <- function(variable_name){paste0('~', round(odds_ratios[[variable_name]]), ' times more likely to survive than die')}

f2 <- 
  function(variable_name, print_name, reference){
    
    tmp <- round((odds_ratios[[variable_name]] - 1)*100) 
    
    paste0(
      'the odds of ', 
      print_name, 
      ' surviving are ~', 
      abs(tmp),
      '%', 
      if_else(sign(tmp) < 0, ' lower', ' higher'),
      ' than the odds for ', 
      reference
      )
    }

```

# (extra slide if time)


Effects:
* Intercept group is `r f1('(Intercept)')`
* `r f2('pclass2nd', '2nd class', '1st class')` 
* `r f2('pclass3rd', '3rd class', '1st class')` 
* `r f2('sexmale', 'males', 'females')` 
* `r f2('age', 'someone 1 year older', 'someone 1 year younger')`  
* `r f2('sibsp', 'someone with 1 more sibling or spouse on board', 'someone with 1 fewer')`
* `r f2('parch', 'someone with 1 more parent or child on board', 'someone with 1 fewer')`

---

# Comparing Performance

We can predict on unseen data to make predictions.

Here is how I made predictions (refer to `.Rmd` for formatting code)

```{r}
tmp_tbl <- 
  testing_data %>% 
  mutate(
    predicted_DT = 
      predict(two_var_DT, newdata = .)[,'survived'], 
    predicted_glm = 
      predict(
        glm_comparison,
        newdata = ., 
        type = 'response'
      ), 
    survived = survived
  )
```

---

# Comparative Accuracy Plot

```{r, include=TRUE,echo=FALSE}
tmp_plt_df <- 
  tmp_tbl %>%
  pivot_longer(
    cols = c(predicted_DT, predicted_glm), 
    names_to = 'model', 
    values_to = 'predicted', 
    names_prefix = 'predicted_', 
  ) %>% 
  mutate(
    predicted_class = 
      if_else(round(predicted) == 1, 'survived', 'died'),
    accuracy = 
      if_else(predicted_class == survived, 'correct', 'incorrect'),
    stringent_accuracy = 
      if_else(is.na(accuracy), 'incorrect', accuracy)
  )

nas_in_glm <- 
  tmp_plt_df %>% 
  group_by(model) %>% 
  count(accuracy) %>% 
  mutate(
    proportion = 
      paste0(
        round(
          (n / sum(n))* 100), 
        '%'
      )
  ) %>% 
  filter(is.na(accuracy)) %>% 
  pull(proportion)

plt_df <- 
  tmp_plt_df %>% 
  group_by(model) %>% 
  count(accuracy) %>% 
  mutate(
    proportion = 
      paste0(
        round(
          (n / sum(n))* 100), 
        '%'
      )
  )

pos_tmp <-
  position_dodge2(preserve = 'single', width = 1)

comp_acc_plt <- 
  plt_df %>% 
  ggplot(
    aes(
      x = accuracy,
      y = n, 
      fill = model
    )
  ) + 
  geom_col(
    position = pos_tmp,
    alpha = 0.6, 
    color = 'black',
  ) +
  geom_label(
    aes(label = proportion), 
    position = pos_tmp
  ) +
  theme_bw(base_size = 18) + 
  labs(
    x = 'Accuracy', 
    y = 'Count',
    title = 'The DT accuracy is higher than the GLM',
    subtitle = 
      paste0(
        'The GLM failed to make a prediction on ~',
        nas_in_glm, 
        ' of cases'
      ),
    
  ) + 
  theme(
    plot.title.position = 'plot', 
    plot.caption.position = 'plot'
  )
```

```{r include=TRUE, echo=FALSE,fig.width=10,fig.height=6,fig.align='center'}
comp_acc_plt
```

---

# In-Depth Accuracy

```{r echo = FALSE, include=TRUE}
glm_dt_tbl <- 
  tmp_plt_df %>%
  mutate(
    type_of_prediction =
      case_when(
        survived == 'died' & predicted_class == 'died' ~ 
          'true negative',
        survived == 'died' & predicted_class == 'survived' ~ 
          'false negative',
        survived == 'survived' & predicted_class == 'died' ~ 
          'false positive',
        survived == 'survived' & predicted_class == 'survived' ~ 
          'true positive'
      )
  ) %>% 
  group_by(model) %>% 
  count(type_of_prediction) %>% 
  mutate(
    percent = paste0(round(100*n/sum(n)), '%')
  )
```

```{r include = TRUE, echo = FALSE}
glm_dt_tbl %>% 
  select(-model) %>% 
  kbl(caption = 'Performance of DT vs. GLM') %>% 
  pack_rows('Decision Tree', 1, 4) %>% 
  pack_rows('Logistic Regression', 5, 9)
```

---

# Cons

Decision trees are great models for

* quickly understanding broad strokes of data
* interpretting complex interactions and nonlinear effects

They have many issues, though:

* Overfitting to the training data
* Unstable tree structure  
* Biased towards selecting variables with more potential split points (i.e., wide ranges)
* Greedy algorithm 
* it finds the best option at each step
* branches are dependent on prior splits

There are numerous ways to overcome these shortcomings, though.

---

# Improving Predictions

Prediction relies on:

1. a model
2. user-specified settings
  * Referred to as **hyperparameters**
  * allows calibration of the model
3. data

Thus, prediction can be improved by improving any one of those.

Throughout the week, we will explore each of these.

---

# Schedule

Feb. 21 we will improve (1) and (2)

* explore a more robust model (random forests) 
* improve models through modifying hyperparameters

Feb. 22 we will

* discuss cross-validation data
* conduct hyperparameter tuning
* reinforce skills from days 1 and 2

---

class: center, middle

# End of day 1