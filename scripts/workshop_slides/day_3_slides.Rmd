---
title: "slides day 3"
author: "Christopher M. Loan, MS"
date: 'February 22, 2022'
output: xaringan::moon_reader
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Workshop Progress

On Day 1, we discussed:
  * what DTs are
  * how to fit DTs
  * how to interprate/visualize DTs
  * how to evaluate DTs

On Day 2, we discussed
  
  * what RFs are
  * how to fit RFs
  * how to evaluate RFs
  * hyperparameters
  * how hyperparameters are used to improve models
  
---

# Looking Ahead

Today

* Establishing benchmarks

* Improving predictions through
  
  * Tuning Hyperparameters
  * Cross-Validation
  
* Interpreting RFs with Partial Dependency Plots (PDPs)

---

# set up

#### Load packages

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(ranger)
library(rsample)
library(dials)
library(kableExtra)
library(emojifont)
library(pdp)
library(vip)
```

#### Load functions

```{r message = FALSE, warning = FALSE}
source(here::here('scripts/functions.R'))
```

#### Set outcome

```{r message = FALSE, warning = FALSE}
outcome_next_year <- 'se_xpd_totl_gd_zs'
```

---

# Technical detail

This is used throughout my code: 
    
    !!sym(outcome_next_year) 
This helps make drastic changes (i.e., what outcome) by simply changing this line: 
    
    outcome_next_year <- NAME_OF_SELECTED_OUTCOME
The same output is generated from

    se_xpd_totl_gd_zs 
instead of 

    !!sym(outcome_next_year)
---

# set up

#### Load Key / Data

```{r}
key <- 
  read.csv(here::here('data/keys', 'key_impute.csv'))

dat <- 
  read.csv(
    here::here(
    'data', 
    'se_asia_imputed_world_bank.csv'
    ))
```

#### Split Data

```{r}
set.seed(022023)
sea_split <- 
  initial_split(dat, strata = country_name, prop = 0.9)
sea_training <- training(sea_split) 
sea_testing <- testing(sea_split) 
```

---

# set up

These variable names are hard to interpret. 

Here's a function that returns a written name from the variable name.

```{r}
pstr <- 
  function(variable = NULL){
    # default returns written outcome
    if(is.null(variable)) {variable <- outcome_next_year}
       
    written_name <- 
      key %>% 
      filter(janitor_version == variable) %>% 
      pull(indicator_name)
    
    return(written_name)
  }
```

Outcome =  <u>next year's</u> `r pstr()`

Units = `r str_extract(pstr(), '\\([^\\)]+\\)')`

---

class: center, middle

# Establishing Benchmarks

(To check if the final model is any good)

---

# Benchmarks

The process of using benchmarks is very easy:

* Predict outcomes for unseen cases with all benchmarks.
* Calculate adequacy of those models (e.g., with mean square error [MSE])
* Determine if our model outperforms these benchmarks

The better the benchmark the final model outperforms, the more confidence we can have in the model.

---

# Today's Benchmarks

I chose 3 benchmarks for example:

* Predicting the mean of training data (low benchmark)
* An auto-regressive linear model (moderate benchmark)
* A RF with default settings (advanced benchmark)

---

# Benchmark 1: Guess the Average (mean)

Guessing the mean has common-sense validity.

Although it is often not thought of as a model, the mean can be thought of as an overly-simple model.

This model has only 1 output: the average of the training set's outcome (i.e., next year's `r pstr(outcome_next_year)`)

Thus, we can take the MSE observed when every observation is predicted to have the mean of the training set for any unseen data


```{r}
predictions_guess.mean <- 
  mean(sea_training[,'outcome_next_year'])

mse_naive_guess.mean <- 
  mse(
    observed_y = sea_testing$outcome_next_year,
    predicted_y = predictions_guess.mean
  )
```

---

# Benchmark 2: Simple Linear Model

Going one step up from the simple average is a simple auto-regressive linear model

Thus, regress  `outcome_next_year` on `r outcome_next_year` 

```{r warning = TRUE}
lm_simple <- 
  lm(
    formula = paste0('outcome_next_year ~ ',  outcome_next_year), 
# equivalent to: 
# formula = outcome_next_year ~ se_xpd_totl_gd_zs,
    data = sea_training,
  ) 
```

---

# Benchmark 3: Default RF

Another (very difficult) benchmark is the random forest set to the default settings. 

```{r}
rf_default <- 
  ranger(
    outcome_next_year ~ ., 
    data = sea_training,
    importance = 'permutation'
  )
```

___

A consideration on the default RF:

* There is nothing wrong with using the default settings of random forests if those settings perform the best.
* RF prediction accuracy has been shown to vary less across tuning, compared to other common ML algorithms

---

# Determining Benchmark MSE

### Step 1 — predict outcomes for unseen data

```{r}
model_preds <- 
  sea_testing %>% 
  transmute(
    # equivalent to: se_xpd_totl_gd_zs
    !!sym(outcome_next_year), 
    predictions_guess.mean = predictions_guess.mean,
    predictions_lm.simple = predict(lm_simple, newdata = .),
    predictions_default.rf = predict(rf_default, .)$prediction
    )
```

---

# Determining Benchmark MSE

### Step 2 — calculate MSE between predictions and observed

```{r}
benchmark_mse <- 
  model_preds %>% 
  summarize(
# use `across` to calculate MSE for 3 models at once
    across(
      .cols = 
        c(predictions_guess.mean, 
          predictions_default.rf,
          predictions_lm.simple), 
      .fns = 
        ~mse(
# equivalent to: observed_y = se_xpd_totl_gd_zs
          observed_y = !!sym(outcome_next_year), 
          predicted_y = .x
        )
    )
  )
```

---

# Benchmarks

The default RF gives great improvement, relative to the other models. 

```{r include = TRUE, echo = FALSE}
benchmark_mse %>% 
  pivot_longer(
    cols = everything(),
    names_prefix = 'predictions_', 
    values_to = 'mse',
    names_to = 'model'
    ) %>% 
  arrange(mse) %>% 
  mutate(rmse = sqrt(mse)) %>% 
  kbl(caption = 'Fit of various benchmarks', digits = 4) %>% 
  footnote('lower is better')

# Note the group MSE and the individual MSE differ after `r sum(sapply(1:20,function(i) grepl(substr(benchmark_mse$predictions_guess.mean,1,i),benchmark_mse$predictions_group.mean)))-2` digits, so pretty much the same in our case.
```

**Hint:** if MSE feels intangible, you can get MSE in the units of the outcome by taking the square root (i.e., the "RMSE"). In other words, the square root of MSE is the average error on each prediction `r pstr()` in % units

---
class: center, middle

# Visualizing Predicting the Mean

```{r echo=FALSE, include=TRUE}
sea_testing %>% 
  ggplot(
    aes(
# equivalent to: se_xpd_totl_gd_zs
      y = !!sym(outcome_next_year),
      x = year
      )
  ) +
  geom_segment(
    aes(xend = year),
    yend = predictions_guess.mean,
    ) +
  geom_point(
    size = 3, alpha = 0.7,
    color = 'cornflowerblue'
  ) +
  geom_hline(
    yintercept = predictions_guess.mean,
    linetype = 2,
    size = 1.2, 
    alpha = 0.4
  ) +
  theme_bw(base_size = 16) +
  facet_wrap(
    vars(
      country_name
    )
  ) +
  labs(
    y = pstr(),
    size = 'Squared Error',
    fill = element_blank(),
    subtitle = 'Dotted line indicates mean %GDP on education', 
    title = 'Demonstrating (mean-only) predictions on testing data'
  ) +
  theme(
    plot.title.position = 'plot',
    legend.position = 'top'
  ) +
  guides(color = 'none')
```

---
class: center, middle

# Visualizing Using Linear Model to Predict

```{r echo=FALSE, include=TRUE}
sea_testing %>% 
  mutate(
      predicted = predict(lm_simple, newdata = .),
      actual =  !!sym(outcome_next_year), # equivalent to: se_xpd_totl_gd_zs
      squared_difference = (actual - predicted)^2
  ) %>% 
  pivot_longer(
    cols = c(predicted, actual)
  ) %>% 
  ggplot(
    aes(
      y = value,
      x = year,
      color = name,
      shape = name,
    )
  ) + 
  geom_line(
    aes(group = year),
    color = 'black'
    ) +
  geom_point(
    size = 3, alpha = 0.7
  ) +
  theme_bw(base_size = 16) +
  facet_wrap(
    vars(
      country_name
    )
  ) +
  labs(
    y = pstr(),
    size = 'Squared\nError',
    fill = element_blank(),
    color = element_blank(),
    shape = element_blank(),
    title = 'Demonstrating linear model predictions on testing data'
  ) +
  theme(
    plot.title.position = 'plot',
    legend.position = 'top'
  ) 
```

---
class: center, middle

# Visualizing Using Default RF to Predict

```{r echo=FALSE, include=TRUE}
sea_testing %>% 
  mutate(
      predicted = predict(rf_default, .)$prediction,
      actual =  !!sym(outcome_next_year), # equivalent to: se_xpd_totl_gd_zs
      squared_difference = (actual - predicted)^2
  ) %>% 
  pivot_longer(
    cols = c(predicted, actual)
  ) %>% 
  ggplot(
    aes(
      y = value,
      x = year,
      color = name,
      shape = name,
    )
  ) +
  geom_line(
    aes(group = year),
    size = 1,
    color = 'black'
    ) +
  geom_point(
    size = 3, alpha = 0.7
  ) +
  theme_bw(base_size = 16) +
  facet_wrap(
    vars(
      country_name
    )
  ) +
  labs(
    y = pstr(),
    fill = element_blank(),
    color = element_blank(),
    shape = element_blank(),
    title = 'Demonstrating RF predictions on testing data'
  ) +
  theme(
    plot.title.position = 'plot',
    legend.position = 'top'
  ) 

```

---

class: center, middle

# Improving Predictions: Hyperparameter Tuning

Now that we've established these benchmarks, let's talk hyperparameters again

---

# Selecting Hyperparameters

Some have called tuning & hyperparameter selection as much of an art as a science.

This is likely true when you're applying the basic methods we're going to cover today. 

Some automated methods are much more systematic:

* particle swarm optimization (PSO)
* genetic algorithm (GA)

Despite being more systematic:

* such methods require deeper knowledge
* most still require human decision-points
* some have their own hyper-hyperparameters!

**This is an entire field; we are only going to discuss <u>grid search</u> today**

---

# Grid Search

Purpose:
* (technical) Calibrate the underlying algorithms to your data
* (practical) Have the most accurate predictions 

Steps:
* make a grid of possible hyperparameters
* train models using those constraints
* estimate accuracy - MSE / classification accuracy / etc. - in predicting the outcome with unseen data for each model 

___

## Wait ... does this sound familiar?

---

# Refresher

When I demonstrated what hyperparameters are, I actually conducted 2 grid searches:
  * `num.trees`
  * `mtry`

This did not include a "grid search" of all possible values

Grid search just means iterating through rows with different combinations of hyperparameters. 

It is called that because the tables you iterate through are laid out like grids (I'll show in one second)

---

# Creating a Grid: Approach

There are many way to make a grid of possible hyperparameter values. We will discuss 2:

* A full, or expanded, grid
* A space-filling grid

Both involve:

* choosing hyperparameters to tune
* determining possible values for each hyperparameters (individually)
* creating a grid (i.e., a table) that finds permutations of these values

space-filling grids: 

* involve another step: determining which of the permutations cover as much of the grid as possible.
* have faster run-times
* uses an algorithm to eliminate redundant permutations

---

# Creating a Grid: Choosing Hyperparemeters & Ranges

```{r include=TRUE, echo=FALSE}
p <- ncol(sea_testing)-1
```

The data has `p = ` `r p` variables available.

* num.trees = [`p*10` = `r p*10`, `p*100` = `r p*100`] by 300
* mtry = [2, `p/2` = `r p/2`] by 2
* min.node.size = [1, 8] by 1



```{r}
tree_sizes <- 
  seq(from = p*10, to = p*50, by = 150)
# [1]  470  620  770  920 1070 1220 1370 1520 1670 1820 1970 2120 2270

mtry_values <- 
  seq(from = 2, to = p/2, by = 1)
# [1]  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23

node_sizes <- 
  seq(from = 1, to = 8, by = 1)
# [1] 1  2  3  4  5  6  7  8
```

---

# Creating a Grid: Expand Combinations

We can return a table that offers all combinations of those 3 vectors with `expand_grid()`.

```{r}
all_params <- 
  expand_grid(
    num.tree = tree_sizes, 
    mtry = mtry_values, 
    node_sizes = node_sizes
    )
```

This is a total of `r nrow(all_params)` combinations. 

These possible combinations of values are often referred to as the "hyperparameter space". 

Each row is one combination hyperparameters or a "hyperparameter state"

---

# Creating a Grid: The Problem

Fitting all `r nrow(all_params)` models will take a large amount of time.

We are only tuning 3 hyperparameters. 

As your grid gets larger:

* the likelihood of missing an optimal hyperparameter state decreases
* computation time increases

---

# Creating a Grid: The Solution 

## Space-filling algorithms 

* attempt to cover the full hyperparameter space by limiting it to "distinct enough" hyperparameter states
* frequently used when tuning with grid search.
* `Maximum entropy` is the one we'll use, but know there are many others


---

# Maximum Entropy Grid

The `dials` package offers a built-in `grid_max_entropy()` function. It requires 2 things:

1. a list of `param` objects (built into `dials`)
  * These must have a minimum and maximum stated 
  * We can use the minimum and maximum from above
  
2. the `size` of your hyperarameter space (i.e., number of unique states)

```{r}
max_entropy_grid <-
  grid_max_entropy(
    # param object 1:  `mtry`
    mtry(range = c(min(mtry_values), max(mtry_values))),
    # param object 2: `num.trees`
    trees(range = c(min(tree_sizes), max(tree_sizes))),
    # param object 3: `min.node.size`
    min_n(range = c(min(node_sizes), max(node_sizes))),
    size = 500
  )
```

---

# Comparison

Max entropy grid has `r nrow(max_entropy_grid)` states, and the full grid has `r nrow(all_params)`

.center[
```{r, echo=FALSE, include=TRUE, warning=FALSE}
stacked_grids <- 
  max_entropy_grid %>% 
  mutate(
    Approach = 'Maximum Entropy Grid'
  ) %>% 
  bind_rows(
    all_params %>% 
      mutate(Approach = 'Full Grid') %>% 
      rename(
        trees = num.tree,
        min_n = node_sizes
      )
  ) 

stacked_grids %>% 
  arrange(Approach) %>% 
  ggplot(
    aes(
      x = mtry, 
      y = trees, 
      color = Approach,
      alpha = Approach
    )
  ) +
  geom_point() + 
  labs(
    y = 'Number of Trees',
    title = 'Coverage of Hyperparmater Space by Approach',
    subtitle = 
      'Maximum entropy grid can cover similar amount of space\nwith fewer models'
  ) +
  theme_bw(base_size = 18) +
  theme(
    legend.position = 'bottom',
    plot.title.position = 'plot'
    ) +
  facet_wrap(vars(min_n)) + 
  scale_alpha_discrete(range = c(0.4, 1)) 
```
]

---

# Fit Model to all Hyperparameter States


```{r eval = FALSE}
rfs_tmp <- vector('list', nrow(max_entropy_grid))
mses_tmp <- vector('double', nrow(max_entropy_grid))
for (i in 1:nrow(max_entropy_grid)){
   
  rfs_tmp[[i]] <- 
    ranger(
      data = sea_training, 
      formula = outcome_next_year ~ ., 
      mtry = max_entropy_grid$mtry[[i]],
      num.trees = max_entropy_grid$trees[[i]],
      min.node.size = max_entropy_grid$min_n[[i]], 
    )
  
  mses_tmp[[i]] <- rfs_tmp[[i]]$prediction.error
  
  print(paste0('iteration #', i, ' complete'))
}
```

---

# Pull Best Value

```{r eval = FALSE}
best_i <- which.min(mses_tmp)

rfs_tmp[[best_i]] <- 
  ranger(
    data = sea_training, 
    formula = outcome_next_year ~ ., 
    mtry = max_entropy_grid$mtry[[best_i]],
    num.trees = max_entropy_grid$trees[[best_i]],
    min.node.size = max_entropy_grid$min_n[[best_i]], 
    importance = 'permutation'
  )


best_fitting <- 
  rfs_tmp[[which.min(mses_tmp)]]
```


```{r eval = FALSE, echo = FALSE}
write_rds(
  best_fitting,
  here::here('data/long_runtime_objects', 'day3_best_fitting_rf.Rds')
  )
rf_tuned <- best_fitting
```

```{r echo = FALSE, include = FALSE}
rf_tuned <- 
  read_rds(
    here::here('data/long_runtime_objects', 'day3_best_fitting_rf.Rds')
    )
```

---

# Comparing Hyperparameters

.pull-left[

#### Tuned

```{r}
rf_tuned$mtry
rf_tuned$num.trees
rf_tuned$min.node.size
```

]

.pull-right[

#### Default

```{r}
rf_default$mtry
rf_default$num.trees
rf_default$min.node.size
```

]

---

# Comparing VIPs

.pull-left[

```{r include = TRUE, echo = FALSE}
vip(rf_tuned) + theme_minimal(base_size = 25) + labs(title = 'VIP from Tuned RF')
```

]

.pull-right[

```{r include = TRUE, echo = FALSE}
vip(rf_default) + theme_minimal(base_size = 25) + labs(title = 'VIP from Default RF')
```

]
---

# Top 5 Important Variables (in text)

.pull-left[

```{r include=TRUE, echo=FALSE}
important_variables_default <- 
  vi(rf_default) %>% 
  arrange(desc(Importance)) %>% 
  slice(1:5) %>% 
  mutate(order = 1:n()) %>% 
  select(janitor_version = Variable, order)


key %>% 
  inner_join(important_variables_default, by = 'janitor_version') %>% 
  arrange(order) %>% 
  select(indicator_name) %>% 
  kbl(caption =  'Tuned RF') 
```

]

.pull-right[

```{r include=TRUE, echo=FALSE}
important_variables <- 
  vi(rf_tuned) %>% 
  arrange(desc(Importance)) %>% 
  slice(1:5) %>% 
  mutate(order = 1:n()) %>% 
  select(janitor_version = Variable, order)

key %>% 
  inner_join(important_variables, by = 'janitor_version') %>% 
  arrange(order) %>% 
  select(indicator_name) %>% 
  kbl(caption =  'Default RF') 

```
]

---

# Evaluate Performance

This model performs well, just barely outperforming the others.

```{r echo=FALSE, include=TRUE}
sea_testing %>% 
  transmute(
# equivalent to: se_xpd_totl_gd_zs
    !!sym(outcome_next_year),
    prediction_tuned.rf = predict(rf_tuned, .)$prediction
  ) %>% 
  summarize(
    mse_rf.tuned = 
      mse(observed_y = !!sym(outcome_next_year), predicted_y = prediction_tuned.rf)
  ) %>% 
  bind_cols(
    benchmark_mse
  ) %>% 
  pivot_longer(
    cols = everything(), 
    names_to = 'Model / Benchmark', 
    values_to = 'Mean Squared Error', 
    names_prefix = 'mse_'
  ) %>% 
  arrange(`Mean Squared Error`) %>% 
  kbl(caption = 'Fit of Models & Benchmarks', digits = 4) %>% 
  footnote('Lower is better')
```

---

class: center, middle

# Interpreting RFs: Partial Dependency Plots

---

## Variable Importance Plots

We've discussed and shown these extensively. 

Useful to know *what* influences the outcome, but does not tell *how* it influences the outcome.

## Partial Dependency Plots (PDPs):

* Used to visualize the relationship between a subset of the features 
* Typically include 1-3 predictors and outcome
* Show how these predictors influence outcome, holding other variables constant
* It is common practice to make PDPs of variables with high Variable Importance scores (i.e., those at top of VIPs)
* Very hard to interpret more than 2-3 features

---

# Selecting Variables for PDP

Variables with top 3 importance scores

```{r}
top_3_importance <- 
  vi(rf_tuned) %>% 
  arrange(desc(Importance)) %>% 
  slice(1:3) %>% 
  mutate(order = 1:n()) %>% 
  select(janitor_version = Variable, order)

top_3_importance %>% 
  left_join(key, by = 'janitor_version') %>% 
  select(importance = order, indicator_name) %>% 
  kbl()
```

---

# Selecting Variables for PDP

```{r}
# get names of ordered importance variables
ordered_imp_vars <- top_3_importance$janitor_version
# make a list of the combinations
combos  <-
  list(
    first_second = ordered_imp_vars[c(1, 2)],
    first_third = ordered_imp_vars[c(1, 3)],
    second_third = ordered_imp_vars[c(2, 3)]
    )

combos
```

---

# Partial Dependency Plots

Heads up: This is computationally intensive! 

```{r, eval=FALSE, include=TRUE, echo=TRUE}
library(pdp)
pds <- vector('list', length(combos))

# fit a pdp for all combos of top 3 variables

for (i in 1:length(combos)){
  pds[[i]] <-  partial(
      rf_tuned, 
      pred.var = combos[[i]]
    ) 
  }
```


```{r, eval = FALSE, include=FALSE, echo=FALSE}
library(pdp)
pds <- vector('list', length(combos))

# fit a pdp for all combos of top 3 variables

for (i in 1:length(combos)){
  pds[[i]] <-  partial(
      rf_tuned, 
      pred.var = combos[[i]]
    ) 
  }

names(pds) <- names(combos)

write_rds(pds, here::here('data/long_runtime_objects', 'day3_tuned_pdps_rf.Rds'))

```

---

```{r include=FALSE, echo=FALSE}
pds <- read_rds(here::here('data/long_runtime_objects', 'day3_tuned_pdps_rf.Rds'))
names(pds) <- names(combos)
```

```{r}
pdp_first_second <- 
  pds$first_second %>% 
  ggplot(
    aes(
      # first combo variable
      x = se_xpd_totl_gd_zs,
      fill = yhat,
      # second combo variable
      y = it_mlt_main,
    )
  ) +
  labs(
    title = 
      "PDP of important variables in predicting next year's",
    subtitle = pstr(),
    y = pstr('it_mlt_main'),
    x = pstr('se_xpd_totl_gd_zs')
  ) +
  geom_tile() + 
  coord_cartesian() +
  theme_bw(base_size = 16)
  
```

---

.left-column[

This heatmap uses brightness to represent `yhat` or the predicted level of the outcome.

x-axis is the most important variable, y-axis is the second most

The most important variable has several discrete jumps (i.e., unique shades left-to-right)

The second most has only 3 unique levels (i.e., unique shades up-and-down)

]

.right-column[

```{r echo = FALSE, include = TRUE}
pdp_first_second
```
]

---

```{r}
pdp_first_third <- 
  pds$first_third %>% 
  ggplot(
    aes(
      # first combo variable
      x = se_xpd_totl_gd_zs,
      fill = yhat,
      # second combo variable
      y = se_pre_enrr,
    )
  ) +
  labs(
    title = 
      "PDP of important variables in predicting next year's",
    subtitle = pstr(),
    y = pstr('se_pre_enrr'),
    x = pstr('se_xpd_totl_gd_zs')
  ) +
  geom_tile() + 
  coord_cartesian() +
  theme_bw(base_size = 16)
  
```

---


.left-column[

x-axis is the most important variable, y-axis is the third most important

The third most important has only 3 unique levels (i.e.,  unique shades up-and-down)

Again, effects are positively associated, but the relationship is not linear 

]

.right-column[

```{r echo = FALSE, include = TRUE}
pdp_first_third
```
]


---

```{r}
pdp_second_third <- 
  pds$second_third %>% 
  ggplot(
    aes(
      # first combo variable
      x = it_mlt_main,
      fill = yhat,
      # second combo variable
      y = se_pre_enrr,
    )
  ) +
  labs(
    title = 
      "PDP of important variables in predicting next year's",
    subtitle = pstr(), 
    y = pstr('se_pre_enrr'),
    x = pstr('it_mlt_main')
  ) +
  geom_tile() + 
  coord_cartesian() +
  theme_bw(base_size = 16)
  
```

---

.left-column[

x-axis is the second most important variable, y-axis is the third most important

The third most important has only 3 unique levels (i.e., unique shades up-and-down)

]

.right-column[

```{r echo = FALSE, include = TRUE}
pdp_second_third
```
]


---

# So, what do we know about the outcome?

Well, `r pstr()` appears to increase along with each of the variables we investigated. 

In several cases, the increase is compounding across variables; however, univariate and multivariate effects had a threshold. 

Unlike linear regression, continuing to increase in one variable did not lead to continuous increases in the outcome. 

In other words, all 3 heatmaps showed that increasing any of these variables lead to an expected increase in the outcome. 

However, the model expects these to be discrete jumps between levels of the outcome, not a linear or even a smooth increase.

---

class: center, middle

# Activity / Discussion

---

# From Model to Understanding

This workshop's focus was on the models, and not the data. 

But let's take a minute to brain storm why some of these variables might be important. 

We can do this in many ways, but I suggest:

* Reading the World Bank's extended definition of these items
* Plotting each variable over time
* Searching the peer-reviewed literature

---

class: center, middle

### Open `workshop_data_guide.html` 

Chrome and Firefox work very well. Safari has trouble with some images. 

Other browsers *should*, but might not, work well.
