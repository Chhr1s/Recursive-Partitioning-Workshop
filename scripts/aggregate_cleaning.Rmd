---
title: "aggregated_cleaning"
author: "Christopher Loan"
date: "2023-01-01"
output: html_document
---

```{r}
# data sources
# https://data.worldbank.org/topic/
```

```{r}
set.seed(4444)
library(tidyverse)
library(janitor)
source(here::here('functions.R'))
```


```{r}
library(tidyverse)
library(janitor)
raw_files <- 
  list.files(here::here('data/raw_data'))

names(raw_files) <- gsub('.csv', '', gsub('wb_all_countries_economic_', '', raw_files))

se_asia <- 
  c(
    'Brunei Darussalam', 
    'Myanmar', 
    'Cambodia', 
    'Timor-Leste', 
    'Indonesia', 
    'Lao PDR', 
    'Malaysia', 
    'Philippines', 
    'Singapore', 
    'Thailand',
    'Vietnam'
  )

dat <- 
  map_dfr(
    .x = raw_files, 
    .f = 
      ~rio::import(
        here::here(
          'data/raw_data', .x
        ),
        skip = 4,
      ) %>% 
      tibble() %>% 
      row_to_names(row_number = 1) %>% 
      clean_names() %>%
      select(
        country_name, 
        indicator_name, 
        indicator_code, 
        starts_with('x')
      ),
    .id = 'import_file_source'
  ) %>% 
  filter(country_name %in% se_asia)


```



```{r}
# make a key so we have all variables easily searchable
key <- 
  dat %>% 
  select(
    indicator_name, 
    indicator_code
    ) %>% 
  mutate(
    janitor_version = janitor::make_clean_names(indicator_code)
  )
```

```{r}
library(sjmisc)
library(parallel)

cleaned <- 
  dat %>% 
  group_by(country_name) %>% 
  nest() %>% 
  mutate(
    data = 
      map(
        .x = data, 
        .f =
        ~{.x %>% 
            mutate(janitor_version = janitor::make_clean_names(indicator_code)) %>% 
            select(-indicator_name, -indicator_code, -import_file_source) %>% 
            relocate(janitor_version, .before = everything()) %>% 
            rotate_df() %>%
            row_to_names(row_number = 1) %>%
            rownames_to_column(var = 'year') %>%
            tibble() %>%
            mutate(
              year = str_extract(string = year, regex('\\d\\d\\d\\d')),
              across(.cols = everything(), ~as.numeric(.x))
            )
        }
      ) 
  ) %>% 
  unnest(data) %>% 
  ungroup()
```

`sl_tlf_cact_zs` = Labor force participation rate, total (% of total population ages 15+) (modeled ILO estimate)

`sl_uem_neet_zs` = Share of youth not in education, employment or training, total (% of youth population)

`se_sec_durs` = Secondary education, duration (years)

```{r}
key
```

```{r}
missingness_counter <-
  cleaned %>%
  group_by(country_name) %>%
  nest() %>%
  mutate(
    data =
      mclapply(
        mc.cores = 9L,
        X = data,
        FUN = function(x) {sum(is.na(x))}
      )
  ) %>%
  unnest(data) %>%
  ungroup()

missingness_counter %>% arrange(desc(data))
```

```{r}
library(janitor)
clean_subset <- 
  cleaned %>% 
  filter(
    country_name %in% se_asia & year >= 2000
    ) %>% 
  remove_constant() %>% 
  remove_empty(which = 'cols') %>% 
  remove_empty(which = 'rows') 

```


```{r}
near_zero_variance <- 
  names(which(apply(clean_subset, 2, var, na.rm = TRUE) < 0.2))

df_to_impute <-
  clean_subset %>% 
  select(
    where(~sum(is.na(.x))/nrow(clean_subset) < 0.25)   & -all_of(near_zero_variance)
  ) 

eligible_vars <- 
  df_to_impute %>% 
  names() 

impute_key <- 
  key %>% 
  filter(
    janitor_version %in% eligible_vars
  ) 

  
# only 7 % missing is good balance of lots of variables 
sum(is.na(df_to_impute))/(nrow(df_to_impute)*ncol(df_to_impute))
# select an outcome with no missingness

full_variables <- 
  df_to_impute %>% 
  # only retain rows with < 10% missingness:
  select(where(~sum(is.na(.x)) < 0.2*nrow(df_to_impute))) %>% 
  # no missingness:
  #select(where(~sum(is.na(.x))==0)) %>% 
  names()


df_to_impute %>% 
  select(all_of(full_variables)) %>% 
  summarize(across(.cols = everything(), .fns = var, na.rm = T)) 

outcome_key <- 
  key %>% 
  filter(
     janitor_version %in% full_variables & 
       !janitor_version %in% near_zero_variance
  )

outcome_key %>% View()
# https://ilostat.ilo.org/resources/concepts-and-definitions/ilo-modelled-estimates/


outcome_descriptor <-
  outcome_key %>% 
  filter(janitor_version == 'ny_gdp_pcap_kd') %>% 
  pull(indicator_name)

# more information at:
# https://ilostat.ilo.org/resources/concepts-and-definitions/ilo-modelled-estimates/

```


we're going to use median impute

```{r eval=FALSE, include=FALSE}
plt <-
  cleaned %>% 
  pivot_longer(
    cols = -year
  ) %>% 
  ggplot(
    aes(
      x = value, 
      color = name, 
      fill = name
    )
  ) + 
  geom_density(
    show.legend = FALSE
  ) 

plt +
  lims(x = c(0, 25))
``` 

```{r}
# differences between mean and median aren't that great (in most cases)
cleaned %>% 
  select(-country_name) %>% 
  summarize(
    across(.cols = everything(),.fns = ~median(.x, na.rm = T) - mean(.x, na.rm = T))
  ) 
```


```{r}
imputed <- 
  df_to_impute %>% 
  group_by(country_name) %>% 
  nest() %>% 
  mutate(
    data = 
      map(
        .x = data, 
        .f = 
          ~.x %>% 
          arrange(year) %>% 
          mutate(
            ## check this
            outcome_next_year = lead(ny_gdp_pcap_kd, order_by = year)
          ) %>% 
        # removes any potential issue from the `lead()` call
        # also removes prediction into covid years
          drop_na(outcome_next_year) %>% 
          mutate(
            across(
              # no missigness in the outcome
              .cols = everything(),
              .fns =
                ~ifelse(
                  is.na(.x),
                  median(.x, na.rm = T),
                  .x)
            )
      )
  )
  ) %>% 
  unnest(data) %>% 
  ungroup() %>% 
  # this keeps out covid issues and also 
  filter(year <= 2018) 
```

```{r}
# rio::export(imputed, here::here('data', 'se_asia_imputed_world_bank.csv'))
# rio::export(key, here::here('data', 'keys/key_full.csv'))
# rio::export(impute_key, here::here('data', 'keys/key_imputed.csv'))
# rio::export(outcome_key, here::here('data', 'keys/key_outcomes_only.csv'))
```

